{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#saelens","title":"SAELens","text":"<p>The SAELens training codebase exists to help researchers:</p> <ul> <li>Train sparse autoencoders.</li> <li>Analyse sparse autoencoders and neural network internals.</li> <li>Generate insights which make it easier to create safe and aligned AI systems.</li> </ul> <p>Please note these docs are in beta. We intend to make them cleaner and more comprehensive over time.</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install sae-lens\n</code></pre>"},{"location":"#loading-sparse-autoencoders-from-huggingface","title":"Loading Sparse Autoencoders from Huggingface","text":""},{"location":"#loading-officially-supported-saes","title":"Loading officially supported SAEs","text":"<p>To load an officially supported sparse autoencoder, you can use <code>SparseAutoencoder.from_pretrained()</code> as below:</p> <p><pre><code>from sae_lens import SparseAutoencoder\n\nlayer = 8 # pick a layer you want.\nsparse_autoencoder = SparseAutoencoder.from_pretrained(\n    \"gpt2-small-res-jb\", f\"blocks.{layer}.hook_resid_pre\"\n)\n</code></pre> Currently, only <code>gpt2-small-res-jb</code> SAEs for the gpt2-small residual-stream are available via this method, but more SAEs will be added soon!</p>"},{"location":"#loading-saes-activationsstore-and-sparsity-from-huggingface","title":"Loading SAEs, ActivationsStore, and Sparsity from Huggingface","text":"<p>For more advanced use-cases like fine-tuning a pre-trained SAE, previously trained sparse autoencoders can be loaded from huggingface with close to single line of code. For more details and performance metrics for these sparse autoencoder, read my blog post. </p> <pre><code>import torch \nfrom sae_lens import LMSparseAutoencoderSessionloader\nfrom huggingface_hub import hf_hub_download\n\nlayer = 8 # pick a layer you want.\nREPO_ID = \"jbloom/GPT2-Small-SAEs\"\nFILENAME = f\"final_sparse_autoencoder_gpt2-small_blocks.{layer}.hook_resid_pre_24576.pt\"\npath = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\nmodel, sparse_autoencoder, activation_store = LMSparseAutoencoderSessionloader.load_session_from_pretrained(\n    path = path\n)\nsparse_autoencoder.eval()\n</code></pre> <p>You can also load the feature sparsity from huggingface. </p> <pre><code>FILENAME = f\"final_sparse_autoencoder_gpt2-small_blocks.{layer}.hook_resid_pre_24576_log_feature_sparsity.pt\"\npath = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\nlog_feature_sparsity = torch.load(path, map_location=sparse_autoencoder.cfg.device)\n</code></pre>"},{"location":"#background","title":"Background","text":"<p>We highly recommend this tutorial.</p>"},{"location":"#code-overview","title":"Code Overview","text":"<p>The codebase contains 2 folders worth caring about:</p> <ul> <li>training: The main body of the code is here. Everything required for training SAEs. </li> <li>analysis: This code is mainly house the feature visualizer code we use to generate dashboards. It was written by Callum McDougal but I've ported it here with permission and edited it to work with a few different activation types. </li> </ul> <p>Some other folders:</p> <ul> <li>tutorials: These aren't well maintained but I'll aim to clean them up soon. </li> <li>tests: When first developing the codebase, I was writing more tests. I have no idea whether they are currently working!</li> </ul>"},{"location":"#loading-a-pretrained-language-model","title":"Loading a Pretrained Language Model","text":"<p>Once your SAE is trained, the final SAE weights will be saved to wandb and are loadable via the session loader. The session loader will return: - The model your SAE was trained on (presumably you're interested in studying this. It's always a HookedTransformer) - Your SAE. - An activations loader: from which you can get randomly sampled activations or batches of tokens from the dataset you used to train the SAE. (more on this in the tutorial)</p> <pre><code>from sae_lens import LMSparseAutoencoderSessionloader\n\npath =\"path/to/sparse_autoencoder.pt\"\nmodel, sparse_autoencoder, activations_loader = LMSparseAutoencoderSessionloader.load_session_from_pretrained(\n    path\n)\n</code></pre>"},{"location":"#tutorials","title":"Tutorials","text":"<p>I wrote a tutorial to show users how to do some basic exploration of their SAE:</p> <ul> <li><code>evaluating_your_sae.ipynb</code>: A quick/dirty notebook showing how to check L0 and Prediction loss with your SAE, as well as showing how to generate interactive dashboards using Callum's reporduction of Anthropics interface.</li> <li><code>logits_lens_with_features.ipynb</code>: A notebook showing how to reproduce the analysis from this LessWrong post.</li> </ul>"},{"location":"#example-dashboard","title":"Example Dashboard","text":"<p>WandB Dashboards provide lots of useful insights while training SAE's. Here's a screenshot from one training run. </p> <p></p>"},{"location":"#citations-and-references","title":"Citations and References:","text":"<p>Research: - Towards Monosemanticy - Sparse Autoencoders Find Highly Interpretable Features in Language Model</p> <p>Reference Implementations: - Neel Nanda - AI-Safety-Foundation. - Arthur Conmy. - Callum McDougall</p>"},{"location":"feature_dashboards/","title":"Feature dashboards","text":""},{"location":"feature_dashboards/#example-output","title":"Example Output","text":"<p>Here's one feature we found in the residual stream of Layer 10 of GPT-2 Small:</p> <p>. Open <code>gpt2_resid_pre10_predict_pronoun_feature.html</code> in your browser to interact with the dashboard (WIP).</p> <p>Note, probably this feature could split into more mono-semantic features in a larger SAE that had been trained for longer. (this was was only about 49152 features trained on 10M tokens from OpenWebText).</p>"},{"location":"installation/","title":"Installation","text":"<p>This package is available on PyPI. You can install it via pip:</p> <pre><code>pip install sae-lens\n</code></pre>"},{"location":"roadmap/","title":"Roadmap","text":""},{"location":"roadmap/#motivation","title":"Motivation","text":"<ul> <li>Accelerate SAE Research: Support fast experimentation to understand SAEs and improve SAE training so we can train SAEs on larger and more diverse models.</li> <li>Make Research like Play: Support research into language model internals via SAEs. Good tooling can make research tremendously exciting and enjoyable. Balancing modifiability and reliability with ease of understanding / access is the name of the game here.</li> <li>Build an awesome community: Mechanistic Interpretability already has an awesome community but as that community grows, it makes sense that there will be niches. I'd love to build a great community around Sparse Autoencoders.</li> </ul>"},{"location":"roadmap/#goals","title":"Goals","text":""},{"location":"roadmap/#sae-training","title":"SAE Training","text":"<p>SAE Training features will fit into a number of categories including:</p> <ul> <li>Making it easy to train SAEs: Training SAEs is hard for a number of reasons and so making it easy for people to train SAEs with relatively little expertise seems like the main way this codebase will create value. </li> <li>Training SAEs on more models: Supporting training of SAEs on more models, architectures, different activations within those models.</li> <li>Being better at training SAEs: Enabling methodological changes which may improve SAE performance as measured by reconstruction loss, Cross Entropy Loss when using reconstructed activation, L1 loss, L0 and interpretability of features as well as improving speed of training or reducing the compute resources required to train SAEs. </li> <li>Being better at measuring SAE Performance: How do we know when SAEs are doing what we want them to? Improving training metrics should allow better decisions about which methods to use and which hyperparameters choices we make.</li> <li>Training SAE variants: People are already training \u201cTranscoders\u201d which map from one activation to another (such as before / after an MLP layer). These can be easily supported with a few changes. Other variants will come in time and </li> </ul>"},{"location":"roadmap/#analysis-with-saes","title":"Analysis with SAEs","text":"<p>Using SAEs to understand neural network internals is an exciting, but complicated task.</p> <ul> <li>Feature-wise Interpretability: This looks something like \"for each feature, have as much knowledge about it as possible\". Part of this will feature dashboard improvements, or supporting better integrations with Neuronpedia.</li> <li>Mechanistic Interpretability: This comprises the more traditional kinds of Mechanistic Interpretability which TransformerLens supports and should be supported by this codebase. Making it easy to patch, ablate or otherwise intervene on features so as to find circuits will likely speed up lots of researchers.</li> </ul>"},{"location":"roadmap/#other-stuff","title":"Other Stuff","text":"<p>I think there are lots of other types of analysis that could be done in the future with SAE features. I've already explored many different types of statistical tests which can reveal interesting properties of features. There are also things like saliency mapping and attribution techniques which it would be nice to support.</p> <ul> <li>Accessibility and Code Quality: The codebase won\u2019t be used if it doesn\u2019t work and it also won\u2019t get used if it\u2019s too hard to understand, modify or read.  Making the code accessible: This involves tasks like turning the code base into a python package.</li> <li>Knowing how the code is supposed to work: Is the code well-documented? This will require docstrings, tutorials and links to related work and publications. Getting aligned on what the code does is critical to sharing a resource like this. </li> <li>Knowing the code works as intended: All code should be tested. Unit tests and acceptance tests are both important.</li> <li>Knowing the code is actually performant: This will ensure code works as intended. However deep learning introduces lots of complexity which makes actually running benchmarks essential to having confidence in the code. </li> </ul>"},{"location":"training_saes/","title":"Training Sparse Autoencoders","text":"<p>Sparse Autoencoders can be intimidating at first but it's fairly simple to train one once you know what each part of the config does. I've created a config class which you instantiate and pass to the runner which will complete your training run and log it's progress to wandb. </p> <p>Let's go through the major components of the config:</p> <ul> <li>Data: SAE's autoencode model activations. We need to specify the model, the part of the models activations we want to autoencode and the dataset the model is operating on when generating those activations. We now automatically detect if that dataset is tokenized and most huggingface datasets should be fine. One slightly annoying detail is that you need to know the dimensionality of those activations when contructing your SAE but you can get that in the transformerlens docs. Any language model in the table from those docs should work. </li> <li>SAE Parameters: Your expansion factor will determine the size of your SAE and the decoder bias initialization method should always be geometric_median or mean. Mean is faster but theoretically sub-optimal. I use another package to get the geometric median and it can be quite slow. </li> <li>Training Parameters: These are most critical. The right L1 coefficient (coefficient in the activation sparsity inducing term in the loss) changes with your learning rate but a good bet would be to use LR 4e-4 and L1 8e-5 for GPT2 small. These will vary for other models and playing around with them / short runs can be helpful. Training batch size of 4096 is standard and I'm not really sure whether there's benefit to playing with it. In theory a larger context size (one accurate to whatever the model was trained with) seems good but it's computationally cheaper to use 128. Learning rate warm up is important to avoid dead neurons. </li> <li>Activation Store Parameters: The activation store shuffles activations from forward passes over samples from your data. The larger it is, the better shuffling you'll get. In theory more shuffling is good. The total training tokens is a very important parameter. The more the better, but you'll often see good results having trained on a few hundred million tokens. Store batch batch size is a function of your gpu and how many forward passes of your model you want to do simultaneously when collecting activations.</li> <li>Dead Neurons / Sparsity Metrics: The config around resampling was more important when we were using resampling to avoid dead neurons (see Anthropic's post on this), but using ghost gradients, the resampling protcol is much simpler. I'd always set ghost grad to True and feature sampling method to None. The feature sampling window effects the dashboard statistics tracking feature occurence and the dead feature window tracks how many forward passes a neuron must not activate before we apply ghost grads to it. </li> <li>WANDB: Fairly straightfoward. Don't set log frequency too high or your dashboard will be slow!</li> <li>Device: I can run this code on my macbook with \"mps\" but mostly do runs with cuda. </li> <li>Dtype: Float16 maybe could work but I had some funky results and have left it at float32 for the time being. </li> <li>Checkpoints: I'd collected checkpoints on runs you care about but turn them off when tuning since it can be slow. </li> </ul> <pre><code>import torch\nimport os \nimport sys \n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n\nfrom sae_lens import LanguageModelSAERunnerConfig, language_model_sae_runner\n\n# NOTE: Refer to training tutorials for updated parameter configurations. \n# Tutorial notebook: https://github.com/jbloomAus/SAELens/blob/main/tutorials/training_a_sparse_autoencoder.ipynb\n# PRs to update docs welcome\ncfg = LanguageModelSAERunnerConfig(\n    # Data Generating Function (Model + Training Distibuion)\n    model_name=\"tiny-stories-1L-21M\",  # our model (more options here: https://neelnanda-io.github.io/TransformerLens/generated/model_properties_table.html)\n    hook_point=\"blocks.0.hook_mlp_out\",  # A valid hook point (see more details here: https://neelnanda-io.github.io/TransformerLens/generated/demos/Main_Demo.html#Hook-Points)\n    hook_point_layer=0,  # Only one layer in the model.\n    d_in=1024,  # the width of the mlp output.\n    dataset_path=\"apollo-research/roneneldan-TinyStories-tokenizer-gpt2\",  # this is a tokenized language dataset on Huggingface for the Tiny Stories corpus.\n    is_dataset_tokenized=True,\n    # SAE Parameters\n    mse_loss_normalization=None,  # We won't normalize the mse loss,\n    expansion_factor=16,  # the width of the SAE. Larger will result in better stats but slower training.\n    b_dec_init_method=\"geometric_median\",  # The geometric median can be used to initialize the decoder weights.\n    # Training Parameters\n    lr=0.0008,  # lower the better, we'll go fairly high to speed up the tutorial.\n    lr_scheduler_name=\"constant\",  # constant learning rate with warmup. Could be better schedules out there.\n    lr_warm_up_steps=10000,  # this can help avoid too many dead features initially.\n    l1_coefficient=0.001,  # will control how sparse the feature activations are\n    lp_norm=1.0,  # the L1 penalty (and not a Lp for p &lt; 1)\n    train_batch_size=4096,\n    context_size=512,  # will control the lenght of the prompts we feed to the model. Larger is better but slower.\n    # Activation Store Parameters\n    n_batches_in_buffer=64,  # controls how many activations we store / shuffle.\n    training_tokens=1_000_000\n    * 50,  # 100 million tokens is quite a few, but we want to see good stats. Get a coffee, come back.\n    store_batch_size=16,\n    # Resampling protocol\n    use_ghost_grads=False,\n    feature_sampling_window=1000,  # this controls our reporting of feature sparsity stats\n    dead_feature_window=1000,  # would effect resampling or ghost grads if we were using it.\n    dead_feature_threshold=1e-4,  # would effect resampling or ghost grads if we were using it.\n    # WANDB\n    log_to_wandb=True,  # always use wandb unless you are just testing code.\n    wandb_project=\"sae_lens_tutorial\",\n    wandb_log_frequency=10,\n    # Misc\n    device=device,\n    seed=42,\n    n_checkpoints=0,\n    checkpoint_path=\"checkpoints\",\n    dtype=torch.float32,\n)\n\nsparse_autoencoder = language_model_sae_runner(cfg)\n</code></pre>"},{"location":"about/citation/","title":"Citation","text":"<pre><code>@misc{bloom2024saetrainingcodebase,\n   title = {SAELens Training\n   author = {Joseph Bloom},\n   year = {2024},\n   howpublished = {\\url{}},\n}}\n</code></pre>"},{"location":"about/contributing/","title":"Contributing","text":"<p>Contributions are welcome! To get setup for development, follow the instructions below.</p>"},{"location":"about/contributing/#setup","title":"Setup","text":"<p>Make sure you have poetry installed, clone the repository, and install dependencies with:</p> <pre><code>poetry install\n</code></pre>"},{"location":"about/contributing/#testing-linting-and-formatting","title":"Testing, Linting, and Formatting","text":"<p>This project uses pytest for testing, flake8 for linting, pyright for type-checking, and black and isort for formatting.</p> <p>If you add new code, it would be greatly appreciated if you could add tests in the <code>tests/unit</code> directory. You can run the tests with:</p> <pre><code>make unit-test\n</code></pre> <p>Before commiting, make sure you format the code with:</p> <pre><code>make format\n</code></pre> <p>Finally, run all CI checks locally with:</p> <pre><code>make check-ci\n</code></pre> <p>If these pass, you're good to go! Open a pull request with your changes.</p>"},{"location":"about/contributing/#documentation","title":"Documentation","text":"<p>This project uses mkdocs for documentation. You can see the docs locally with:</p> <p><pre><code>make docs-serve\n</code></pre> If you make changes to code which requires updating documentation, it would be greatly appreciated if you could update the docs as well.</p>"},{"location":"reference/language_models/","title":"Language Models","text":"<p>Most of this is just copied over from Arthur's code and slightly simplified: https://github.com/ArthurConmy/sae/blob/main/sae/model.py</p>"},{"location":"reference/language_models/#sae_lens.training.lm_runner.language_model_sae_runner","title":"<code>language_model_sae_runner(cfg)</code>","text":"Source code in <code>sae_lens/training/lm_runner.py</code> <pre><code>def language_model_sae_runner(cfg: LanguageModelSAERunnerConfig):\n    \"\"\" \"\"\"\n    training_run_state = None\n    train_contexts = None\n\n    if cfg.resume:\n        try:\n            checkpoint_path = cfg.get_resume_checkpoint_path()\n            model = load_model(\n                model_class_name=cfg.model_class_name,\n                model_name=cfg.model_name,\n                device=cfg.device,\n            )\n            model.to(cfg.device)\n            (\n                training_run_state,\n                activations_loader,\n                sparse_autoencoder,\n                train_contexts,\n            ) = load_checkpoint(\n                checkpoint_path=checkpoint_path,\n                cfg=cfg,\n                model=model,\n                batch_size=cfg.train_batch_size_tokens,\n            )\n        # no checkpoints found, don't resume\n        except FileNotFoundError:\n            print(traceback.format_exc())\n            print(\"failed to find checkpoint to resume from, setting resume to False\")\n            cfg.resume = False\n\n    if not cfg.resume:\n        if cfg.from_pretrained_path is not None:\n            (\n                model,\n                sparse_autoencoder,\n                activations_loader,\n            ) = LMSparseAutoencoderSessionloader.load_pretrained_sae(\n                cfg.from_pretrained_path\n            )\n            cfg = sparse_autoencoder.cfg\n        else:\n            loader = LMSparseAutoencoderSessionloader(cfg)\n            model, sparse_autoencoder, activations_loader = (\n                loader.load_sae_training_group_session()\n            )\n\n    if cfg.log_to_wandb:\n        resume = None\n        if cfg.resume:\n            resume = \"allow\"\n        wandb.init(\n            project=cfg.wandb_project,\n            config=cast(Any, cfg),\n            name=cfg.run_name,\n            resume=resume,\n            id=cfg.wandb_id,\n        )\n\n    # Compile model and SAE\n    # torch.compile can provide significant speedups (10-20% in testing)\n    # using max-autotune gives the best speedups but:\n    # (a) increases VRAM usage,\n    # (b) can't be used on both SAE and LM (some issue with cudagraphs), and\n    # (c) takes some time to compile\n    # optimal settings seem to be:\n    # use max-autotune on SAE and max-autotune-no-cudagraphs on LM\n    # (also pylance seems to really hate this)\n    if cfg.compile_llm:\n        model = torch.compile(\n            model,  # pyright: ignore [reportPossiblyUnboundVariable]\n            mode=cfg.llm_compilation_mode,\n        )\n\n    if cfg.compile_sae:\n        for (\n            k\n        ) in (\n            sparse_autoencoder.autoencoders.keys()  # pyright: ignore [reportPossiblyUnboundVariable]\n        ):\n            sae = sparse_autoencoder.autoencoders[  # pyright: ignore [reportPossiblyUnboundVariable]\n                k\n            ]\n            sae = torch.compile(sae, mode=cfg.sae_compilation_mode)\n            sparse_autoencoder.autoencoders[k] = sae  # type: ignore # pyright: ignore [reportPossiblyUnboundVariable]\n\n    # train SAE\n    sparse_autoencoder = train_sae_group_on_language_model(\n        model=model,  # pyright: ignore [reportPossiblyUnboundVariable] # type: ignore\n        sae_group=sparse_autoencoder,  # pyright: ignore [reportPossiblyUnboundVariable]\n        activation_store=activations_loader,  # pyright: ignore [reportPossiblyUnboundVariable]\n        train_contexts=train_contexts,\n        training_run_state=training_run_state,\n        batch_size=cfg.train_batch_size_tokens,\n        n_checkpoints=cfg.n_checkpoints,\n        feature_sampling_window=cfg.feature_sampling_window,\n        use_wandb=cfg.log_to_wandb,\n        wandb_log_frequency=cfg.wandb_log_frequency,\n        eval_every_n_wandb_logs=cfg.eval_every_n_wandb_logs,\n        autocast=cfg.autocast,\n        n_eval_batches=cfg.n_eval_batches,\n        eval_batch_size_prompts=cfg.eval_batch_size_prompts,\n    ).sae_group\n\n    if cfg.log_to_wandb:\n        wandb.finish()\n\n    return sparse_autoencoder\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.train_sae_on_language_model.SAETrainContext","title":"<code>SAETrainContext</code>  <code>dataclass</code>","text":"<p>Context to track during training for a single SAE</p> Source code in <code>sae_lens/training/train_sae_on_language_model.py</code> <pre><code>@dataclass\nclass SAETrainContext:\n    \"\"\"\n    Context to track during training for a single SAE\n    \"\"\"\n\n    act_freq_scores: torch.Tensor\n    n_forward_passes_since_fired: torch.Tensor\n    n_frac_active_tokens: int\n    optimizer: Optimizer\n    lr_scheduler: LRScheduler\n    l1_scheduler: L1Scheduler\n    finetuning: bool = False\n\n    @property\n    def feature_sparsity(self) -&gt; torch.Tensor:\n        return self.act_freq_scores / self.n_frac_active_tokens\n\n    @property\n    def log_feature_sparsity(self) -&gt; torch.Tensor:\n        return _log_feature_sparsity(self.feature_sparsity)\n\n    def begin_finetuning(self, sae: SparseAutoencoder):\n\n        # finetuning method should be set in the config\n        # if not, then we don't finetune\n        if not isinstance(sae.cfg.finetuning_method, str):\n            return\n\n        for name, param in sae.named_parameters():\n            if name in FINETUNING_PARAMETERS[sae.cfg.finetuning_method]:\n                param.requires_grad = True\n            else:\n                param.requires_grad = False\n\n        self.finetuning = True\n\n    def state_dict(self) -&gt; dict[str, torch.Tensor]:\n        state_dict = {}\n        for attr in fields(self):\n            value = getattr(self, attr.name)\n            # serializable fields\n            if hasattr(value, \"state_dict\"):\n                state_dict[attr.name] = value.state_dict()\n            else:\n                state_dict[attr.name] = value\n        return state_dict\n\n    @classmethod\n    def load(cls, path: str, sae: SparseAutoencoder, total_training_steps: int):\n        with open(path, \"rb\") as f:\n            state_dict = pickle.load(f)\n        attached_ctx = _build_train_context(\n            sae=sae, total_training_steps=total_training_steps\n        )\n        for attr in fields(attached_ctx):\n            value = getattr(attached_ctx, attr.name)\n            # optimizer and scheduler, this attaches them properly\n            if hasattr(value, \"state_dict\"):\n                value.load_state_dict(state_dict[attr.name])\n                state_dict[attr.name] = value\n        ctx = cls(**state_dict)  # pyright: ignore [reportArgumentType]\n        # if fine tuning, we need to set sae requires grad properly\n        if ctx.finetuning:\n            ctx.begin_finetuning(sae=sae)\n        return ctx\n\n    def save(self, path: str):\n        with open(path, \"wb\") as f:\n            pickle.dump(self.state_dict(), f)\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.train_sae_on_language_model.SAETrainingRunState","title":"<code>SAETrainingRunState</code>  <code>dataclass</code>","text":"<p>Training run state for all SAES includes n_training_steps n_training_tokens started_fine_tuning and rng states</p> Source code in <code>sae_lens/training/train_sae_on_language_model.py</code> <pre><code>@dataclass\nclass SAETrainingRunState:\n    \"\"\"\n    Training run state for all SAES\n    includes n_training_steps\n    n_training_tokens\n    started_fine_tuning\n    and rng states\n    \"\"\"\n\n    n_training_steps: int = 0\n    n_training_tokens: int = 0\n    started_fine_tuning: bool = False\n    checkpoint_paths: list[str] = field(default_factory=list)\n    torch_state: Optional[torch.Tensor] = None\n    torch_cuda_state: Optional[list[torch.Tensor]] = None\n    numpy_state: Optional[\n        dict[str, Any]\n        | tuple[str, np.ndarray[Any, np.dtype[np.uint32]], int, int, float]\n    ] = None\n    random_state: Optional[Any] = None\n\n    def __post_init__(self):\n        if self.torch_state is None:\n            self.torch_state = torch.get_rng_state()\n        if self.torch_cuda_state is None:\n            self.torch_cuda_state = torch.cuda.get_rng_state_all()\n        if self.numpy_state is None:\n            self.numpy_state = np.random.get_state()\n        if self.random_state is None:\n            self.random_state = random.getstate()\n\n    def set_random_state(self):\n        assert self.torch_state is not None\n        torch.random.set_rng_state(self.torch_state)\n        assert self.torch_cuda_state is not None\n        torch.cuda.set_rng_state_all(self.torch_cuda_state)\n        assert self.numpy_state is not None\n        np.random.set_state(self.numpy_state)\n        assert self.random_state is not None\n        random.setstate(self.random_state)\n\n    @classmethod\n    def load(cls, path: str):\n        with open(path, \"rb\") as f:\n            attr_dict = pickle.load(f)\n        return cls(**attr_dict)\n\n    def save(self, path: str):\n        attr_dict = {**self.__dict__}\n        with open(path, \"wb\") as f:\n            pickle.dump(attr_dict, f)\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.train_sae_on_language_model.train_sae_on_language_model","title":"<code>train_sae_on_language_model(model, sae_group, activation_store, batch_size=1024, n_checkpoints=0, feature_sampling_window=1000, dead_feature_threshold=1e-08, use_wandb=False, wandb_log_frequency=50, eval_every_n_wandb_logs=100, autocast=False, n_eval_batches=10, eval_batch_size_prompts=None)</code>","text":"<p>@deprecated Use <code>train_sae_group_on_language_model</code> instead. This method is kept for backward compatibility.</p> Source code in <code>sae_lens/training/train_sae_on_language_model.py</code> <pre><code>def train_sae_on_language_model(\n    model: HookedRootModule,\n    sae_group: SparseAutoencoderDictionary,\n    activation_store: ActivationsStore,\n    batch_size: int = 1024,\n    n_checkpoints: int = 0,\n    feature_sampling_window: int = 1000,  # how many training steps between resampling the features / considiring neurons dead\n    dead_feature_threshold: float = 1e-8,  # how infrequently a feature has to be active to be considered dead\n    use_wandb: bool = False,\n    wandb_log_frequency: int = 50,\n    eval_every_n_wandb_logs: int = 100,\n    autocast: bool = False,\n    n_eval_batches: int = 10,\n    eval_batch_size_prompts: int | None = None,\n) -&gt; SparseAutoencoderDictionary:\n    \"\"\"\n    @deprecated Use `train_sae_group_on_language_model` instead. This method is kept for backward compatibility.\n    \"\"\"\n    return train_sae_group_on_language_model(\n        model=model,\n        sae_group=sae_group,\n        activation_store=activation_store,\n        batch_size=batch_size,\n        n_checkpoints=n_checkpoints,\n        feature_sampling_window=feature_sampling_window,\n        use_wandb=use_wandb,\n        wandb_log_frequency=wandb_log_frequency,\n        eval_every_n_wandb_logs=eval_every_n_wandb_logs,\n        autocast=autocast,\n        n_eval_batches=n_eval_batches,\n        eval_batch_size_prompts=eval_batch_size_prompts,\n    ).sae_group\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.sparse_autoencoder.SparseAutoencoder","title":"<code>SparseAutoencoder</code>","text":"<p>             Bases: <code>HookedRootModule</code></p> Source code in <code>sae_lens/training/sparse_autoencoder.py</code> <pre><code>class SparseAutoencoder(HookedRootModule):\n    \"\"\" \"\"\"\n\n    l1_coefficient: float\n    lp_norm: float\n    d_sae: int\n    use_ghost_grads: bool\n    normalize_sae_decoder: bool\n    hook_point_layer: int\n    dtype: torch.dtype\n    device: str | torch.device\n    noise_scale: float\n    activation_fn: Callable[[torch.Tensor], torch.Tensor]\n\n    def __init__(\n        self,\n        cfg: LanguageModelSAERunnerConfig,\n    ):\n        super().__init__()\n        self.cfg = cfg\n        self.d_in = cfg.d_in\n        if not isinstance(self.d_in, int):\n            raise ValueError(\n                f\"d_in must be an int but was {self.d_in=}; {type(self.d_in)=}\"\n            )\n        assert cfg.d_sae is not None  # keep pyright happy\n        # lists are valid only for SAEGroup cfg, not SAE cfg vals\n        assert not isinstance(cfg.l1_coefficient, list)\n        assert not isinstance(cfg.lp_norm, list)\n        assert not isinstance(cfg.lr, list)\n        assert not isinstance(cfg.lr_scheduler_name, list)\n        assert not isinstance(cfg.lr_warm_up_steps, list)\n        assert not isinstance(cfg.use_ghost_grads, list)\n        assert not isinstance(cfg.hook_point_layer, list)\n        assert (\n            \"{layer}\" not in cfg.hook_point\n        ), \"{layer} must be replaced with the actual layer number in SAE cfg\"\n\n        self.d_sae = cfg.d_sae\n        self.l1_coefficient = cfg.l1_coefficient\n        self.lp_norm = cfg.lp_norm\n        self.dtype = cfg.dtype\n        self.device = cfg.device\n        self.use_ghost_grads = cfg.use_ghost_grads\n        self.normalize_sae_decoder = cfg.normalize_sae_decoder\n        self.hook_point_layer = cfg.hook_point_layer\n        self.noise_scale = cfg.noise_scale\n        self.activation_fn = get_activation_fn(cfg.activation_fn)\n\n        if self.cfg.scale_sparsity_penalty_by_decoder_norm:\n            self.get_sparsity_loss_term = self.get_sparsity_loss_term_decoder_norm\n        else:\n            self.get_sparsity_loss_term = self.get_sparsity_loss_term_standard\n\n        self.initialize_weights()\n\n        self.hook_sae_in = HookPoint()\n        self.hook_hidden_pre = HookPoint()\n        self.hook_hidden_post = HookPoint()\n        self.hook_sae_out = HookPoint()\n\n        self.setup()  # Required for `HookedRootModule`s\n\n    def initialize_weights(self):\n        \"\"\"\n        Wrapped around weight initialization code to make init cleaner.\n\n        \"\"\"\n\n        # no config changes encoder bias init for now.\n        self.b_enc = nn.Parameter(\n            torch.zeros(self.d_sae, dtype=self.dtype, device=self.device)\n        )\n\n        # Start with the default init strategy:\n        self.W_dec = nn.Parameter(\n            torch.nn.init.kaiming_uniform_(\n                torch.empty(self.d_sae, self.d_in, dtype=self.dtype, device=self.device)\n            )\n        )\n        if self.cfg.decoder_orthogonal_init:\n            self.W_dec.data = nn.init.orthogonal_(self.W_dec.data.T).T\n\n        elif self.cfg.decoder_heuristic_init:\n            self.W_dec = nn.Parameter(\n                torch.rand(self.d_sae, self.d_in, dtype=self.dtype, device=self.device)\n            )\n            self.initialize_decoder_norm_constant_norm()\n\n        elif self.cfg.normalize_sae_decoder:\n            self.set_decoder_norm_to_unit_norm()\n\n        self.W_enc = nn.Parameter(\n            torch.nn.init.kaiming_uniform_(\n                torch.empty(self.d_in, self.d_sae, dtype=self.dtype, device=self.device)\n            )\n        )\n\n        # Then we intialize the encoder weights (either as the transpose of decoder or not)\n        if self.cfg.init_encoder_as_decoder_transpose:\n            self.W_enc.data = self.W_dec.data.T.clone().contiguous()\n        else:\n            self.W_enc = nn.Parameter(\n                torch.nn.init.kaiming_uniform_(\n                    torch.empty(\n                        self.d_in, self.d_sae, dtype=self.dtype, device=self.device\n                    )\n                )\n            )\n\n        if self.normalize_sae_decoder:\n            with torch.no_grad():\n                # Anthropic normalize this to have unit columns\n                self.set_decoder_norm_to_unit_norm()\n\n        # methdods which change b_dec as a function of the dataset are implemented after init.\n        self.b_dec = nn.Parameter(\n            torch.zeros(self.d_in, dtype=self.dtype, device=self.device)\n        )\n\n        # scaling factor for fine-tuning (not to be used in initial training)\n        self.scaling_factor = nn.Parameter(\n            torch.ones(self.d_sae, dtype=self.dtype, device=self.device)\n        )\n\n    def encode(\n        self, x: Float[torch.Tensor, \"... d_in\"]\n    ) -&gt; Float[torch.Tensor, \"... d_sae\"]:\n        feature_acts, _ = self._encode_with_hidden_pre(x)\n        return feature_acts\n\n    def _encode_with_hidden_pre(\n        self, x: Float[torch.Tensor, \"... d_in\"]\n    ) -&gt; tuple[Float[torch.Tensor, \"... d_sae\"], Float[torch.Tensor, \"... d_sae\"]]:\n        \"\"\"Encodes input activation tensor x into an SAE feature activation tensor.\"\"\"\n        # move x to correct dtype\n        x = x.to(self.dtype)\n        sae_in = self.hook_sae_in(\n            x - (self.b_dec * self.cfg.apply_b_dec_to_input)\n        )  # Remove decoder bias as per Anthropic\n\n        hidden_pre = self.hook_hidden_pre(\n            einops.einsum(\n                sae_in,\n                self.W_enc,\n                \"... d_in, d_in d_sae -&gt; ... d_sae\",\n            )\n            + self.b_enc\n        )\n        noisy_hidden_pre = hidden_pre\n        if self.noise_scale &gt; 0:\n            noise = torch.randn_like(hidden_pre) * self.noise_scale\n            noisy_hidden_pre = hidden_pre + noise\n        feature_acts = self.hook_hidden_post(self.activation_fn(noisy_hidden_pre))\n\n        return feature_acts, hidden_pre\n\n    def decode(\n        self, feature_acts: Float[torch.Tensor, \"... d_sae\"]\n    ) -&gt; Float[torch.Tensor, \"... d_in\"]:\n        \"\"\"Decodes SAE feature activation tensor into a reconstructed input activation tensor.\"\"\"\n        sae_out = self.hook_sae_out(\n            einops.einsum(\n                feature_acts\n                * self.scaling_factor,  # need to make sure this handled when loading old models.\n                self.W_dec,\n                \"... d_sae, d_sae d_in -&gt; ... d_in\",\n            )\n            + self.b_dec\n        )\n        return sae_out\n\n    def get_sparsity_loss_term_standard(\n        self, feature_acts: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Sparsity loss term calculated as the L1 norm of the feature activations.\n        \"\"\"\n        sparsity = feature_acts.norm(p=self.lp_norm, dim=-1)\n        return sparsity\n\n    def get_sparsity_loss_term_decoder_norm(\n        self, feature_acts: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Sparsity loss term for decoder norm regularization.\n        \"\"\"\n        weighted_feature_acts = feature_acts * self.W_dec.norm(dim=1)\n        sparsity = weighted_feature_acts.norm(\n            p=self.lp_norm, dim=-1\n        )  # sum over the feature dimension\n        return sparsity\n\n    def forward(\n        self, x: torch.Tensor, dead_neuron_mask: torch.Tensor | None = None\n    ) -&gt; ForwardOutput:\n\n        feature_acts, hidden_pre = self._encode_with_hidden_pre(x)\n        sae_out = self.decode(feature_acts)\n\n        # add config for whether l2 is normalized:\n        per_item_mse_loss = _per_item_mse_loss_with_target_norm(\n            sae_out, x, self.cfg.mse_loss_normalization\n        )\n\n        # gate on config and training so evals is not slowed down.\n        if (\n            self.use_ghost_grads\n            and self.training\n            and dead_neuron_mask is not None\n            and dead_neuron_mask.sum() &gt; 0\n        ):\n            ghost_grad_loss = self.calculate_ghost_grad_loss(\n                x=x,\n                sae_out=sae_out,\n                per_item_mse_loss=per_item_mse_loss,\n                hidden_pre=hidden_pre,\n                dead_neuron_mask=dead_neuron_mask,\n            )\n        else:\n            ghost_grad_loss = 0\n\n        mse_loss = per_item_mse_loss.sum(dim=-1).mean()\n        sparsity = self.get_sparsity_loss_term(feature_acts)\n        l1_loss = (self.l1_coefficient * sparsity).mean()\n        loss = mse_loss + l1_loss + ghost_grad_loss\n\n        return ForwardOutput(\n            sae_out=sae_out,\n            feature_acts=feature_acts,\n            loss=loss,\n            mse_loss=mse_loss,\n            l1_loss=l1_loss,\n            ghost_grad_loss=ghost_grad_loss,\n        )\n\n    @torch.no_grad()\n    def initialize_b_dec_with_precalculated(self, origin: torch.Tensor):\n        out = torch.tensor(origin, dtype=self.dtype, device=self.device)\n        self.b_dec.data = out\n\n    @torch.no_grad()\n    def initialize_b_dec_with_mean(self, all_activations: torch.Tensor):\n        previous_b_dec = self.b_dec.clone().cpu()\n        out = all_activations.mean(dim=0)\n\n        previous_distances = torch.norm(all_activations - previous_b_dec, dim=-1)\n        distances = torch.norm(all_activations - out, dim=-1)\n\n        print(\"Reinitializing b_dec with mean of activations\")\n        print(\n            f\"Previous distances: {previous_distances.median(0).values.mean().item()}\"\n        )\n        print(f\"New distances: {distances.median(0).values.mean().item()}\")\n\n        self.b_dec.data = out.to(self.dtype).to(self.device)\n\n    @torch.no_grad()\n    def set_decoder_norm_to_unit_norm(self):\n        self.W_dec.data /= torch.norm(self.W_dec.data, dim=1, keepdim=True)\n\n    @torch.no_grad()\n    def initialize_decoder_norm_constant_norm(self, norm: float = 0.1):\n        \"\"\"\n        A heuristic proceedure inspired by:\n        https://transformer-circuits.pub/2024/april-update/index.html#training-saes\n        \"\"\"\n        # TODO: Parameterise this as a function of m and n\n\n        # ensure W_dec norms at unit norm\n        self.W_dec.data /= torch.norm(self.W_dec.data, dim=1, keepdim=True)\n        self.W_dec.data *= 0.1  # will break tests but do this for now.\n\n    @torch.no_grad()\n    def remove_gradient_parallel_to_decoder_directions(self):\n        \"\"\"\n        Update grads so that they remove the parallel component\n            (d_sae, d_in) shape\n        \"\"\"\n        assert self.W_dec.grad is not None  # keep pyright happy\n\n        parallel_component = einops.einsum(\n            self.W_dec.grad,\n            self.W_dec.data,\n            \"d_sae d_in, d_sae d_in -&gt; d_sae\",\n        )\n        self.W_dec.grad -= einops.einsum(\n            parallel_component,\n            self.W_dec.data,\n            \"d_sae, d_sae d_in -&gt; d_sae d_in\",\n        )\n\n    def save_model_legacy(self, path: str):\n        \"\"\"\n        Basic save function for the model. Saves the model's state_dict and the config used to train it.\n        \"\"\"\n\n        # check if path exists\n        folder = os.path.dirname(path)\n        os.makedirs(folder, exist_ok=True)\n\n        state_dict = {\"cfg\": self.cfg, \"state_dict\": self.state_dict()}\n\n        if path.endswith(\".pt\"):\n            torch.save(state_dict, path)\n        else:\n            raise ValueError(\n                f\"Unexpected file extension: {path}, supported extensions are .pt and .pkl.gz\"\n            )\n\n        print(f\"Saved model to {path}\")\n\n    def save_model(self, path: str, sparsity: Optional[torch.Tensor] = None):\n\n        if not os.path.exists(path):\n            os.mkdir(path)\n\n        # generate the weights\n        save_file(self.state_dict(), f\"{path}/{SAE_WEIGHTS_PATH}\")\n\n        # save the config\n        config = {\n            **self.cfg.__dict__,\n            # some args may not be serializable by default\n            \"dtype\": str(self.cfg.dtype),\n            \"device\": str(self.cfg.device),\n        }\n\n        with open(f\"{path}/{SAE_CFG_PATH}\", \"w\") as f:\n            json.dump(config, f)\n\n        if sparsity is not None:\n            sparsity_in_dict = {\"sparsity\": sparsity}\n            save_file(sparsity_in_dict, f\"{path}/{SPARSITY_PATH}\")  # type: ignore\n\n    @classmethod\n    def load_from_pretrained_legacy(cls, path: str):\n        \"\"\"\n        Load function for the model. Loads the model's state_dict and the config used to train it.\n        This method can be called directly on the class, without needing an instance.\n        \"\"\"\n\n        # Ensure the file exists\n        if not os.path.isfile(path):\n            raise FileNotFoundError(f\"No file found at specified path: {path}\")\n\n        # Load the state dictionary\n        if path.endswith(\".pt\"):\n            try:\n                if torch.backends.mps.is_available():\n                    state_dict = torch.load(\n                        path,\n                        map_location=\"mps\",\n                        pickle_module=BackwardsCompatiblePickleClass,\n                    )\n                    state_dict[\"cfg\"].device = \"mps\"\n                else:\n                    state_dict = torch.load(\n                        path, pickle_module=BackwardsCompatiblePickleClass\n                    )\n            except Exception as e:\n                raise IOError(f\"Error loading the state dictionary from .pt file: {e}\")\n        elif path.endswith(\".pkl.gz\"):\n            try:\n                with gzip.open(path, \"rb\") as f:\n                    state_dict = pickle.load(f)\n            except Exception as e:\n                raise IOError(\n                    f\"Error loading the state dictionary from .pkl.gz file: {e}\"\n                )\n        elif path.endswith(\".pkl\"):\n            try:\n                with open(path, \"rb\") as f:\n                    state_dict = pickle.load(f)\n            except Exception as e:\n                raise IOError(f\"Error loading the state dictionary from .pkl file: {e}\")\n        else:\n            raise ValueError(\n                f\"Unexpected file extension: {path}, supported extensions are .pt, .pkl, and .pkl.gz\"\n            )\n\n        # Ensure the loaded state contains both 'cfg' and 'state_dict'\n        if \"cfg\" not in state_dict or \"state_dict\" not in state_dict:\n            raise ValueError(\n                \"The loaded state dictionary must contain 'cfg' and 'state_dict' keys\"\n            )\n\n        # Create an instance of the class using the loaded configuration\n        instance = cls(cfg=state_dict[\"cfg\"])\n        if \"scaling_factor\" not in state_dict[\"state_dict\"]:\n            assert isinstance(instance.cfg.d_sae, int)\n            state_dict[\"state_dict\"][\"scaling_factor\"] = torch.ones(\n                instance.cfg.d_sae, dtype=instance.cfg.dtype, device=instance.cfg.device\n            )\n        instance.load_state_dict(state_dict[\"state_dict\"], strict=True)\n\n        return instance\n\n    @classmethod\n    def load_from_pretrained(\n        cls, path: str, device: str = \"cpu\"\n    ) -&gt; \"SparseAutoencoder\":\n\n        config_path = os.path.join(path, \"cfg.json\")\n        weight_path = os.path.join(path, \"sae_weights.safetensors\")\n\n        cfg, state_dict = load_pretrained_sae_lens_sae_components(\n            config_path, weight_path, device\n        )\n\n        sae = cls(cfg)\n        sae.load_state_dict(state_dict)\n\n        return sae\n\n    @classmethod\n    def from_pretrained(\n        cls, release: str, sae_id: str, device: str = \"cpu\"\n    ) -&gt; \"SparseAutoencoder\":\n        \"\"\"\n\n        Load a pretrained SAE from the Hugging Face model hub.\n\n        Args:\n            release: The release name. This will be mapped to a huggingface repo id based on the pretrained_saes.yaml file.\n            id: The id of the SAE to load. This will be mapped to a path in the huggingface repo.\n            device: The device to load the SAE on.\n\n        \"\"\"\n\n        # get sae directory\n        sae_directory = get_pretrained_saes_directory()\n\n        # get the repo id and path to the SAE\n        if release not in sae_directory:\n            raise ValueError(\n                f\"Release {release} not found in pretrained SAEs directory.\"\n            )\n        if sae_id not in sae_directory[release].saes_map:\n            raise ValueError(f\"ID {sae_id} not found in release {release}.\")\n        sae_info = sae_directory[release]\n        hf_repo_id = sae_info.repo_id\n        hf_path = sae_info.saes_map[sae_id]\n\n        conversion_loader_name = sae_info.conversion_func or \"sae_lens\"\n        if conversion_loader_name not in NAMED_PRETRAINED_SAE_LOADERS:\n            raise ValueError(\n                f\"Conversion func {conversion_loader_name} not found in NAMED_PRETRAINED_SAE_LOADERS.\"\n            )\n        conversion_loader = NAMED_PRETRAINED_SAE_LOADERS[conversion_loader_name]\n\n        cfg, state_dict = conversion_loader(\n            repo_id=hf_repo_id,\n            folder_name=hf_path,\n            device=device,\n            force_download=False,\n        )\n\n        sae = cls(cfg)\n        sae.load_state_dict(state_dict)\n\n        return sae\n\n    def get_name(self):\n        sae_name = f\"sparse_autoencoder_{self.cfg.model_name}_{self.cfg.hook_point}_{self.cfg.d_sae}\"\n        return sae_name\n\n    def calculate_ghost_grad_loss(\n        self,\n        x: torch.Tensor,\n        sae_out: torch.Tensor,\n        per_item_mse_loss: torch.Tensor,\n        hidden_pre: torch.Tensor,\n        dead_neuron_mask: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        # 1.\n        residual = x - sae_out\n        l2_norm_residual = torch.norm(residual, dim=-1)\n\n        # 2.\n        feature_acts_dead_neurons_only = torch.exp(hidden_pre[:, dead_neuron_mask])\n        ghost_out = feature_acts_dead_neurons_only @ self.W_dec[dead_neuron_mask, :]\n        l2_norm_ghost_out = torch.norm(ghost_out, dim=-1)\n        norm_scaling_factor = l2_norm_residual / (1e-6 + l2_norm_ghost_out * 2)\n        ghost_out = ghost_out * norm_scaling_factor[:, None].detach()\n\n        # 3.\n        per_item_mse_loss_ghost_resid = _per_item_mse_loss_with_target_norm(\n            ghost_out, residual.detach(), self.cfg.mse_loss_normalization\n        )\n        mse_rescaling_factor = (\n            per_item_mse_loss / (per_item_mse_loss_ghost_resid + 1e-6)\n        ).detach()\n        per_item_mse_loss_ghost_resid = (\n            mse_rescaling_factor * per_item_mse_loss_ghost_resid\n        )\n\n        return per_item_mse_loss_ghost_resid.mean()\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.sparse_autoencoder.SparseAutoencoder.decode","title":"<code>decode(feature_acts)</code>","text":"<p>Decodes SAE feature activation tensor into a reconstructed input activation tensor.</p> Source code in <code>sae_lens/training/sparse_autoencoder.py</code> <pre><code>def decode(\n    self, feature_acts: Float[torch.Tensor, \"... d_sae\"]\n) -&gt; Float[torch.Tensor, \"... d_in\"]:\n    \"\"\"Decodes SAE feature activation tensor into a reconstructed input activation tensor.\"\"\"\n    sae_out = self.hook_sae_out(\n        einops.einsum(\n            feature_acts\n            * self.scaling_factor,  # need to make sure this handled when loading old models.\n            self.W_dec,\n            \"... d_sae, d_sae d_in -&gt; ... d_in\",\n        )\n        + self.b_dec\n    )\n    return sae_out\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.sparse_autoencoder.SparseAutoencoder.from_pretrained","title":"<code>from_pretrained(release, sae_id, device='cpu')</code>  <code>classmethod</code>","text":"<p>Load a pretrained SAE from the Hugging Face model hub.</p> <p>Parameters:</p> Name Type Description Default <code>release</code> <code>str</code> <p>The release name. This will be mapped to a huggingface repo id based on the pretrained_saes.yaml file.</p> required <code>id</code> <p>The id of the SAE to load. This will be mapped to a path in the huggingface repo.</p> required <code>device</code> <code>str</code> <p>The device to load the SAE on.</p> <code>'cpu'</code> Source code in <code>sae_lens/training/sparse_autoencoder.py</code> <pre><code>@classmethod\ndef from_pretrained(\n    cls, release: str, sae_id: str, device: str = \"cpu\"\n) -&gt; \"SparseAutoencoder\":\n    \"\"\"\n\n    Load a pretrained SAE from the Hugging Face model hub.\n\n    Args:\n        release: The release name. This will be mapped to a huggingface repo id based on the pretrained_saes.yaml file.\n        id: The id of the SAE to load. This will be mapped to a path in the huggingface repo.\n        device: The device to load the SAE on.\n\n    \"\"\"\n\n    # get sae directory\n    sae_directory = get_pretrained_saes_directory()\n\n    # get the repo id and path to the SAE\n    if release not in sae_directory:\n        raise ValueError(\n            f\"Release {release} not found in pretrained SAEs directory.\"\n        )\n    if sae_id not in sae_directory[release].saes_map:\n        raise ValueError(f\"ID {sae_id} not found in release {release}.\")\n    sae_info = sae_directory[release]\n    hf_repo_id = sae_info.repo_id\n    hf_path = sae_info.saes_map[sae_id]\n\n    conversion_loader_name = sae_info.conversion_func or \"sae_lens\"\n    if conversion_loader_name not in NAMED_PRETRAINED_SAE_LOADERS:\n        raise ValueError(\n            f\"Conversion func {conversion_loader_name} not found in NAMED_PRETRAINED_SAE_LOADERS.\"\n        )\n    conversion_loader = NAMED_PRETRAINED_SAE_LOADERS[conversion_loader_name]\n\n    cfg, state_dict = conversion_loader(\n        repo_id=hf_repo_id,\n        folder_name=hf_path,\n        device=device,\n        force_download=False,\n    )\n\n    sae = cls(cfg)\n    sae.load_state_dict(state_dict)\n\n    return sae\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.sparse_autoencoder.SparseAutoencoder.get_sparsity_loss_term_decoder_norm","title":"<code>get_sparsity_loss_term_decoder_norm(feature_acts)</code>","text":"<p>Sparsity loss term for decoder norm regularization.</p> Source code in <code>sae_lens/training/sparse_autoencoder.py</code> <pre><code>def get_sparsity_loss_term_decoder_norm(\n    self, feature_acts: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"\n    Sparsity loss term for decoder norm regularization.\n    \"\"\"\n    weighted_feature_acts = feature_acts * self.W_dec.norm(dim=1)\n    sparsity = weighted_feature_acts.norm(\n        p=self.lp_norm, dim=-1\n    )  # sum over the feature dimension\n    return sparsity\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.sparse_autoencoder.SparseAutoencoder.get_sparsity_loss_term_standard","title":"<code>get_sparsity_loss_term_standard(feature_acts)</code>","text":"<p>Sparsity loss term calculated as the L1 norm of the feature activations.</p> Source code in <code>sae_lens/training/sparse_autoencoder.py</code> <pre><code>def get_sparsity_loss_term_standard(\n    self, feature_acts: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"\n    Sparsity loss term calculated as the L1 norm of the feature activations.\n    \"\"\"\n    sparsity = feature_acts.norm(p=self.lp_norm, dim=-1)\n    return sparsity\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.sparse_autoencoder.SparseAutoencoder.initialize_decoder_norm_constant_norm","title":"<code>initialize_decoder_norm_constant_norm(norm=0.1)</code>","text":"<p>A heuristic proceedure inspired by: https://transformer-circuits.pub/2024/april-update/index.html#training-saes</p> Source code in <code>sae_lens/training/sparse_autoencoder.py</code> <pre><code>@torch.no_grad()\ndef initialize_decoder_norm_constant_norm(self, norm: float = 0.1):\n    \"\"\"\n    A heuristic proceedure inspired by:\n    https://transformer-circuits.pub/2024/april-update/index.html#training-saes\n    \"\"\"\n    # TODO: Parameterise this as a function of m and n\n\n    # ensure W_dec norms at unit norm\n    self.W_dec.data /= torch.norm(self.W_dec.data, dim=1, keepdim=True)\n    self.W_dec.data *= 0.1  # will break tests but do this for now.\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.sparse_autoencoder.SparseAutoencoder.initialize_weights","title":"<code>initialize_weights()</code>","text":"<p>Wrapped around weight initialization code to make init cleaner.</p> Source code in <code>sae_lens/training/sparse_autoencoder.py</code> <pre><code>def initialize_weights(self):\n    \"\"\"\n    Wrapped around weight initialization code to make init cleaner.\n\n    \"\"\"\n\n    # no config changes encoder bias init for now.\n    self.b_enc = nn.Parameter(\n        torch.zeros(self.d_sae, dtype=self.dtype, device=self.device)\n    )\n\n    # Start with the default init strategy:\n    self.W_dec = nn.Parameter(\n        torch.nn.init.kaiming_uniform_(\n            torch.empty(self.d_sae, self.d_in, dtype=self.dtype, device=self.device)\n        )\n    )\n    if self.cfg.decoder_orthogonal_init:\n        self.W_dec.data = nn.init.orthogonal_(self.W_dec.data.T).T\n\n    elif self.cfg.decoder_heuristic_init:\n        self.W_dec = nn.Parameter(\n            torch.rand(self.d_sae, self.d_in, dtype=self.dtype, device=self.device)\n        )\n        self.initialize_decoder_norm_constant_norm()\n\n    elif self.cfg.normalize_sae_decoder:\n        self.set_decoder_norm_to_unit_norm()\n\n    self.W_enc = nn.Parameter(\n        torch.nn.init.kaiming_uniform_(\n            torch.empty(self.d_in, self.d_sae, dtype=self.dtype, device=self.device)\n        )\n    )\n\n    # Then we intialize the encoder weights (either as the transpose of decoder or not)\n    if self.cfg.init_encoder_as_decoder_transpose:\n        self.W_enc.data = self.W_dec.data.T.clone().contiguous()\n    else:\n        self.W_enc = nn.Parameter(\n            torch.nn.init.kaiming_uniform_(\n                torch.empty(\n                    self.d_in, self.d_sae, dtype=self.dtype, device=self.device\n                )\n            )\n        )\n\n    if self.normalize_sae_decoder:\n        with torch.no_grad():\n            # Anthropic normalize this to have unit columns\n            self.set_decoder_norm_to_unit_norm()\n\n    # methdods which change b_dec as a function of the dataset are implemented after init.\n    self.b_dec = nn.Parameter(\n        torch.zeros(self.d_in, dtype=self.dtype, device=self.device)\n    )\n\n    # scaling factor for fine-tuning (not to be used in initial training)\n    self.scaling_factor = nn.Parameter(\n        torch.ones(self.d_sae, dtype=self.dtype, device=self.device)\n    )\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.sparse_autoencoder.SparseAutoencoder.load_from_pretrained_legacy","title":"<code>load_from_pretrained_legacy(path)</code>  <code>classmethod</code>","text":"<p>Load function for the model. Loads the model's state_dict and the config used to train it. This method can be called directly on the class, without needing an instance.</p> Source code in <code>sae_lens/training/sparse_autoencoder.py</code> <pre><code>@classmethod\ndef load_from_pretrained_legacy(cls, path: str):\n    \"\"\"\n    Load function for the model. Loads the model's state_dict and the config used to train it.\n    This method can be called directly on the class, without needing an instance.\n    \"\"\"\n\n    # Ensure the file exists\n    if not os.path.isfile(path):\n        raise FileNotFoundError(f\"No file found at specified path: {path}\")\n\n    # Load the state dictionary\n    if path.endswith(\".pt\"):\n        try:\n            if torch.backends.mps.is_available():\n                state_dict = torch.load(\n                    path,\n                    map_location=\"mps\",\n                    pickle_module=BackwardsCompatiblePickleClass,\n                )\n                state_dict[\"cfg\"].device = \"mps\"\n            else:\n                state_dict = torch.load(\n                    path, pickle_module=BackwardsCompatiblePickleClass\n                )\n        except Exception as e:\n            raise IOError(f\"Error loading the state dictionary from .pt file: {e}\")\n    elif path.endswith(\".pkl.gz\"):\n        try:\n            with gzip.open(path, \"rb\") as f:\n                state_dict = pickle.load(f)\n        except Exception as e:\n            raise IOError(\n                f\"Error loading the state dictionary from .pkl.gz file: {e}\"\n            )\n    elif path.endswith(\".pkl\"):\n        try:\n            with open(path, \"rb\") as f:\n                state_dict = pickle.load(f)\n        except Exception as e:\n            raise IOError(f\"Error loading the state dictionary from .pkl file: {e}\")\n    else:\n        raise ValueError(\n            f\"Unexpected file extension: {path}, supported extensions are .pt, .pkl, and .pkl.gz\"\n        )\n\n    # Ensure the loaded state contains both 'cfg' and 'state_dict'\n    if \"cfg\" not in state_dict or \"state_dict\" not in state_dict:\n        raise ValueError(\n            \"The loaded state dictionary must contain 'cfg' and 'state_dict' keys\"\n        )\n\n    # Create an instance of the class using the loaded configuration\n    instance = cls(cfg=state_dict[\"cfg\"])\n    if \"scaling_factor\" not in state_dict[\"state_dict\"]:\n        assert isinstance(instance.cfg.d_sae, int)\n        state_dict[\"state_dict\"][\"scaling_factor\"] = torch.ones(\n            instance.cfg.d_sae, dtype=instance.cfg.dtype, device=instance.cfg.device\n        )\n    instance.load_state_dict(state_dict[\"state_dict\"], strict=True)\n\n    return instance\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.sparse_autoencoder.SparseAutoencoder.remove_gradient_parallel_to_decoder_directions","title":"<code>remove_gradient_parallel_to_decoder_directions()</code>","text":"<p>Update grads so that they remove the parallel component     (d_sae, d_in) shape</p> Source code in <code>sae_lens/training/sparse_autoencoder.py</code> <pre><code>@torch.no_grad()\ndef remove_gradient_parallel_to_decoder_directions(self):\n    \"\"\"\n    Update grads so that they remove the parallel component\n        (d_sae, d_in) shape\n    \"\"\"\n    assert self.W_dec.grad is not None  # keep pyright happy\n\n    parallel_component = einops.einsum(\n        self.W_dec.grad,\n        self.W_dec.data,\n        \"d_sae d_in, d_sae d_in -&gt; d_sae\",\n    )\n    self.W_dec.grad -= einops.einsum(\n        parallel_component,\n        self.W_dec.data,\n        \"d_sae, d_sae d_in -&gt; d_sae d_in\",\n    )\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.sparse_autoencoder.SparseAutoencoder.save_model_legacy","title":"<code>save_model_legacy(path)</code>","text":"<p>Basic save function for the model. Saves the model's state_dict and the config used to train it.</p> Source code in <code>sae_lens/training/sparse_autoencoder.py</code> <pre><code>def save_model_legacy(self, path: str):\n    \"\"\"\n    Basic save function for the model. Saves the model's state_dict and the config used to train it.\n    \"\"\"\n\n    # check if path exists\n    folder = os.path.dirname(path)\n    os.makedirs(folder, exist_ok=True)\n\n    state_dict = {\"cfg\": self.cfg, \"state_dict\": self.state_dict()}\n\n    if path.endswith(\".pt\"):\n        torch.save(state_dict, path)\n    else:\n        raise ValueError(\n            f\"Unexpected file extension: {path}, supported extensions are .pt and .pkl.gz\"\n        )\n\n    print(f\"Saved model to {path}\")\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.activations_store.ActivationsStore","title":"<code>ActivationsStore</code>","text":"<p>Class for streaming tokens and generating and storing activations while training SAEs.</p> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>class ActivationsStore:\n    \"\"\"\n    Class for streaming tokens and generating and storing activations\n    while training SAEs.\n    \"\"\"\n\n    model: HookedRootModule\n    dataset: HfDataset\n    cached_activations_path: str | None\n    tokens_column: Literal[\"tokens\", \"input_ids\", \"text\"]\n    hook_point_head_index: int | None\n    _dataloader: Iterator[Any] | None = None\n    _storage_buffer: torch.Tensor | None = None\n\n    @classmethod\n    def from_config(\n        cls,\n        model: HookedRootModule,\n        cfg: LanguageModelSAERunnerConfig | CacheActivationsRunnerConfig,\n        dataset: HfDataset | None = None,\n    ) -&gt; \"ActivationsStore\":\n        cached_activations_path = cfg.cached_activations_path\n        # set cached_activations_path to None if we're not using cached activations\n        if (\n            isinstance(cfg, LanguageModelSAERunnerConfig)\n            and not cfg.use_cached_activations\n        ):\n            cached_activations_path = None\n        return cls(\n            model=model,\n            dataset=dataset or cfg.dataset_path,\n            streaming=cfg.streaming,\n            hook_point=cfg.hook_point,\n            hook_point_layers=listify(cfg.hook_point_layer),\n            hook_point_head_index=cfg.hook_point_head_index,\n            context_size=cfg.context_size,\n            d_in=cfg.d_in,\n            n_batches_in_buffer=cfg.n_batches_in_buffer,\n            total_training_tokens=cfg.training_tokens,\n            store_batch_size_prompts=cfg.store_batch_size_prompts,\n            train_batch_size_tokens=cfg.train_batch_size_tokens,\n            prepend_bos=cfg.prepend_bos,\n            normalize_activations=cfg.normalize_activations,\n            device=cfg.device,\n            dtype=cfg.dtype,\n            cached_activations_path=cached_activations_path,\n            model_kwargs=cfg.model_kwargs,\n        )\n\n    def __init__(\n        self,\n        model: HookedRootModule,\n        dataset: HfDataset | str,\n        streaming: bool,\n        hook_point: str,\n        hook_point_layers: list[int],\n        hook_point_head_index: int | None,\n        context_size: int,\n        d_in: int,\n        n_batches_in_buffer: int,\n        total_training_tokens: int,\n        store_batch_size_prompts: int,\n        train_batch_size_tokens: int,\n        prepend_bos: bool,\n        normalize_activations: bool,\n        device: str | torch.device,\n        dtype: str | torch.dtype,\n        cached_activations_path: str | None = None,\n        model_kwargs: dict[str, Any] | None = None,\n    ):\n        self.model = model\n        if model_kwargs is None:\n            model_kwargs = {}\n        self.model_kwargs = model_kwargs\n        self.dataset = (\n            load_dataset(dataset, split=\"train\", streaming=streaming)\n            if isinstance(dataset, str)\n            else dataset\n        )\n        self.hook_point = hook_point\n        self.hook_point_layers = hook_point_layers\n        self.hook_point_head_index = hook_point_head_index\n        self.context_size = context_size\n        self.d_in = d_in\n        self.n_batches_in_buffer = n_batches_in_buffer\n        self.total_training_tokens = total_training_tokens\n        self.store_batch_size_prompts = store_batch_size_prompts\n        self.train_batch_size_tokens = train_batch_size_tokens\n        self.prepend_bos = prepend_bos\n        self.normalize_activations = normalize_activations\n        self.device = device\n        self.dtype = dtype\n        self.cached_activations_path = cached_activations_path\n\n        self.n_dataset_processed = 0\n        self.iterable_dataset = iter(self.dataset)\n\n        # Check if dataset is tokenized\n        dataset_sample = next(self.iterable_dataset)\n\n        # check if it's tokenized\n        if \"tokens\" in dataset_sample.keys():\n            self.is_dataset_tokenized = True\n            self.tokens_column = \"tokens\"\n        elif \"input_ids\" in dataset_sample.keys():\n            self.is_dataset_tokenized = True\n            self.tokens_column = \"input_ids\"\n        elif \"text\" in dataset_sample.keys():\n            self.is_dataset_tokenized = False\n            self.tokens_column = \"text\"\n        else:\n            raise ValueError(\n                \"Dataset must have a 'tokens', 'input_ids', or 'text' column.\"\n            )\n        self.iterable_dataset = iter(self.dataset)  # Reset iterator after checking\n\n        self.check_cached_activations_against_config()\n\n        # TODO add support for \"mixed loading\" (ie use cache until you run out, then switch over to streaming from HF)\n\n    def check_cached_activations_against_config(self):\n\n        if self.cached_activations_path is not None:  # EDIT: load from multi-layer acts\n            assert self.cached_activations_path is not None  # keep pyright happy\n            # Sanity check: does the cache directory exist?\n            assert os.path.exists(\n                self.cached_activations_path\n            ), f\"Cache directory {self.cached_activations_path} does not exist. Consider double-checking your dataset, model, and hook names.\"\n\n            self.next_cache_idx = 0  # which file to open next\n            self.next_idx_within_buffer = 0  # where to start reading from in that file\n\n            # Check that we have enough data on disk\n            first_buffer = self.load_buffer(\n                f\"{self.cached_activations_path}/{self.next_cache_idx}.safetensors\"\n            )\n\n            buffer_size_on_disk = first_buffer.shape[0]\n            n_buffers_on_disk = len(os.listdir(self.cached_activations_path))\n\n            # Note: we're assuming all files have the same number of tokens\n            # (which seems reasonable imo since that's what our script does)\n            n_activations_on_disk = buffer_size_on_disk * n_buffers_on_disk\n            assert (\n                n_activations_on_disk &gt;= self.total_training_tokens\n            ), f\"Only {n_activations_on_disk/1e6:.1f}M activations on disk, but total_training_tokens is {self.total_training_tokens/1e6:.1f}M.\"\n\n    def apply_norm_scaling_factor(self, activations: torch.Tensor) -&gt; torch.Tensor:\n        return activations * self.estimated_norm_scaling_factor\n\n    def get_norm_scaling_factor(self, activations: torch.Tensor) -&gt; torch.Tensor:\n        return (self.d_in**0.5) / activations.norm(dim=-1).mean()\n\n    @property\n    def storage_buffer(self) -&gt; torch.Tensor:\n        if self._storage_buffer is None:\n            self._storage_buffer = self.get_buffer(self.n_batches_in_buffer // 2)\n\n        return self._storage_buffer\n\n    @property\n    def dataloader(self) -&gt; Iterator[Any]:\n        if self._dataloader is None:\n            self._dataloader = self.get_data_loader()\n        return self._dataloader\n\n    def get_batch_tokens(self, batch_size: int | None = None):\n        \"\"\"\n        Streams a batch of tokens from a dataset.\n        \"\"\"\n        if not batch_size:\n            batch_size = self.store_batch_size_prompts\n        context_size = self.context_size\n        device = self.device\n\n        batch_tokens = torch.zeros(\n            size=(0, context_size), device=device, dtype=torch.long, requires_grad=False\n        )\n\n        current_batch = []\n        current_length = 0\n\n        # pbar = tqdm(total=batch_size, desc=\"Filling batches\")\n        while batch_tokens.shape[0] &lt; batch_size:\n            tokens = self._get_next_dataset_tokens()\n            token_len = tokens.shape[0]\n\n            # TODO: Fix this so that we are limiting how many tokens we get from the same context.\n            assert self.model.tokenizer is not None  # keep pyright happy\n            while token_len &gt; 0 and batch_tokens.shape[0] &lt; batch_size:\n                # Space left in the current batch\n                space_left = context_size - current_length\n\n                # If the current tokens fit entirely into the remaining space\n                if token_len &lt;= space_left:\n                    current_batch.append(tokens[:token_len])\n                    current_length += token_len\n                    break\n\n                else:\n                    # Take as much as will fit\n                    current_batch.append(tokens[:space_left])\n\n                    # Remove used part, add BOS\n                    tokens = tokens[space_left:]\n                    token_len -= space_left\n\n                    # only add BOS if it's not already the first token\n                    if self.prepend_bos:\n                        bos_token_id_tensor = torch.tensor(\n                            [self.model.tokenizer.bos_token_id],\n                            device=tokens.device,\n                            dtype=torch.long,\n                        )\n                        if tokens[0] != bos_token_id_tensor:\n                            tokens = torch.cat(\n                                (\n                                    bos_token_id_tensor,\n                                    tokens,\n                                ),\n                                dim=0,\n                            )\n                            token_len += 1\n                    current_length = context_size\n\n                # If a batch is full, concatenate and move to next batch\n                if current_length == context_size:\n                    full_batch = torch.cat(current_batch, dim=0)\n                    batch_tokens = torch.cat(\n                        (batch_tokens, full_batch.unsqueeze(0)), dim=0\n                    )\n                    current_batch = []\n                    current_length = 0\n\n            # pbar.n = batch_tokens.shape[0]\n            # pbar.refresh()\n        return batch_tokens[:batch_size]\n\n    def get_activations(self, batch_tokens: torch.Tensor):\n        \"\"\"\n        Returns activations of shape (batches, context, num_layers, d_in)\n\n        d_in may result from a concatenated head dimension.\n        \"\"\"\n        layers = self.hook_point_layers\n        act_names = [self.hook_point.format(layer=layer) for layer in layers]\n        hook_point_max_layer = max(layers)\n\n        layerwise_activations = self.model.run_with_cache(\n            batch_tokens,\n            names_filter=act_names,\n            stop_at_layer=hook_point_max_layer + 1,\n            prepend_bos=self.prepend_bos,\n            **self.model_kwargs,\n        )[1]\n\n        n_batches, n_context = batch_tokens.shape\n\n        stacked_activations = torch.zeros(\n            (n_batches, n_context, len(layers), self.d_in)\n        )\n\n        for i, act_name in enumerate(act_names):\n\n            if self.hook_point_head_index is not None:\n                stacked_activations[:, :, i] = layerwise_activations[act_name][\n                    :, :, self.hook_point_head_index\n                ]\n            elif (\n                layerwise_activations[act_names[0]].ndim &gt; 3\n            ):  # if we have a head dimension\n                stacked_activations[:, :, i] = layerwise_activations[act_name].view(\n                    n_batches, n_context, -1\n                )\n            else:\n                stacked_activations[:, :, i] = layerwise_activations[act_name]\n\n        return stacked_activations\n\n    def get_buffer(self, n_batches_in_buffer: int) -&gt; torch.Tensor:\n        context_size = self.context_size\n        batch_size = self.store_batch_size_prompts\n        d_in = self.d_in\n        total_size = batch_size * n_batches_in_buffer\n        num_layers = len(self.hook_point_layers)  # Number of hook points or layers\n\n        if self.cached_activations_path is not None:\n            # Load the activations from disk\n            buffer_size = total_size * context_size\n            # Initialize an empty tensor with an additional dimension for layers\n            new_buffer = torch.zeros(\n                (buffer_size, num_layers, d_in),\n                dtype=self.dtype,  # type: ignore\n                device=self.device,\n            )\n            n_tokens_filled = 0\n\n            # Assume activations for different layers are stored separately and need to be combined\n            while n_tokens_filled &lt; buffer_size:\n                if not os.path.exists(\n                    f\"{self.cached_activations_path}/{self.next_cache_idx}.safetensors\"\n                ):\n                    print(\n                        \"\\n\\nWarning: Ran out of cached activation files earlier than expected.\"\n                    )\n                    print(\n                        f\"Expected to have {buffer_size} activations, but only found {n_tokens_filled}.\"\n                    )\n                    if buffer_size % self.total_training_tokens != 0:\n                        print(\n                            \"This might just be a rounding error \u2014 your batch_size * n_batches_in_buffer * context_size is not divisible by your total_training_tokens\"\n                        )\n                    print(f\"Returning a buffer of size {n_tokens_filled} instead.\")\n                    print(\"\\n\\n\")\n                    new_buffer = new_buffer[:n_tokens_filled, ...]\n                    return new_buffer\n\n                activations = self.load_buffer(\n                    f\"{self.cached_activations_path}/{self.next_cache_idx}.safetensors\"\n                )\n                taking_subset_of_file = False\n                if n_tokens_filled + activations.shape[0] &gt; buffer_size:\n                    activations = activations[: buffer_size - n_tokens_filled, ...]\n                    taking_subset_of_file = True\n\n                new_buffer[\n                    n_tokens_filled : n_tokens_filled + activations.shape[0], ...\n                ] = activations\n\n                if taking_subset_of_file:\n                    self.next_idx_within_buffer = activations.shape[0]\n                else:\n                    self.next_cache_idx += 1\n                    self.next_idx_within_buffer = 0\n\n                n_tokens_filled += activations.shape[0]\n\n            return new_buffer\n\n        refill_iterator = range(0, batch_size * n_batches_in_buffer, batch_size)\n        # Initialize empty tensor buffer of the maximum required size with an additional dimension for layers\n        new_buffer = torch.zeros(\n            (total_size, context_size, num_layers, d_in),\n            dtype=self.dtype,  # type: ignore\n            device=self.device,\n        )\n\n        for refill_batch_idx_start in refill_iterator:\n            refill_batch_tokens = self.get_batch_tokens()\n            refill_activations = self.get_activations(refill_batch_tokens)\n            new_buffer[\n                refill_batch_idx_start : refill_batch_idx_start + batch_size, ...\n            ] = refill_activations\n\n            # pbar.update(1)\n\n        new_buffer = new_buffer.reshape(-1, num_layers, d_in)\n        new_buffer = new_buffer[torch.randperm(new_buffer.shape[0])]\n\n        # every buffer should be normalized:\n        if self.normalize_activations:\n            try:\n                # check if we've already estimated the norm scaling factor\n                assert self.estimated_norm_scaling_factor is not None\n            except AttributeError:\n                # if we haven't estimated it yet, do so.\n                assert (\n                    new_buffer.shape[0] &gt; 3e5\n                ), \"Warning: Storage buffer is too small to calculate norm scaling factor and expect it to be reliable.\"\n                self.estimated_norm_scaling_factor = self.get_norm_scaling_factor(\n                    new_buffer\n                )\n\n            new_buffer = self.apply_norm_scaling_factor(new_buffer)\n\n        return new_buffer\n\n    def save_buffer(self, buffer: torch.Tensor, path: str):\n        \"\"\"\n        Used by cached activations runner to save a buffer to disk.\n        For reuse by later workflows.\n        \"\"\"\n        save_file({\"activations\": buffer}, path)\n\n    def load_buffer(self, path: str) -&gt; torch.Tensor:\n\n        with safe_open(path, framework=\"pt\", device=str(self.device)) as f:  # type: ignore\n            buffer = f.get_tensor(\"activations\")\n        return buffer\n\n    def get_data_loader(\n        self,\n    ) -&gt; Iterator[Any]:\n        \"\"\"\n        Return a torch.utils.dataloader which you can get batches from.\n\n        Should automatically refill the buffer when it gets to n % full.\n        (better mixing if you refill and shuffle regularly).\n\n        \"\"\"\n\n        batch_size = self.train_batch_size_tokens\n\n        # 1. # create new buffer by mixing stored and new buffer\n        mixing_buffer = torch.cat(\n            [self.get_buffer(self.n_batches_in_buffer // 2), self.storage_buffer],\n            dim=0,\n        )\n\n        mixing_buffer = mixing_buffer[torch.randperm(mixing_buffer.shape[0])]\n\n        # 2.  put 50 % in storage\n        self._storage_buffer = mixing_buffer[: mixing_buffer.shape[0] // 2]\n\n        # 3. put other 50 % in a dataloader\n        dataloader = iter(\n            DataLoader(\n                # TODO: seems like a typing bug?\n                cast(Any, mixing_buffer[mixing_buffer.shape[0] // 2 :]),\n                batch_size=batch_size,\n                shuffle=True,\n            )\n        )\n\n        return dataloader\n\n    def next_batch(self):\n        \"\"\"\n        Get the next batch from the current DataLoader.\n        If the DataLoader is exhausted, refill the buffer and create a new DataLoader.\n        \"\"\"\n        try:\n            # Try to get the next batch\n            return next(self.dataloader)\n        except StopIteration:\n            # If the DataLoader is exhausted, create a new one\n            self._dataloader = self.get_data_loader()\n            return next(self.dataloader)\n\n    @classmethod\n    def load(\n        cls,\n        file_path: str,\n        model: HookedRootModule,\n        cfg: LanguageModelSAERunnerConfig | CacheActivationsRunnerConfig,\n        dataset: HfDataset | None = None,\n    ):\n        activation_store = cls.from_config(model=model, cfg=cfg, dataset=dataset)\n\n        state_dict = load_file(file_path)\n        if \"storage_buffer\" in state_dict.keys():\n            activation_store._storage_buffer = state_dict[\"storage_buffer\"].to(\n                cfg.device\n            )\n        n_dataset_processed = state_dict[\"n_dataset_processed\"].item()\n        # fastforward data\n        pbar = tqdm.tqdm(\n            total=n_dataset_processed - activation_store.n_dataset_processed,\n            desc=\"Fast forwarding data\",\n        )\n        while activation_store.n_dataset_processed &lt; n_dataset_processed:\n            next(activation_store.iterable_dataset)\n            pbar.update(1)\n            activation_store.n_dataset_processed += 1\n        return activation_store\n\n    def state_dict(self) -&gt; dict[str, torch.Tensor]:\n        result = {\n            \"n_dataset_processed\": torch.tensor(self.n_dataset_processed),\n        }\n        if self._storage_buffer is not None:  # first time might be None\n            result[\"storage_buffer\"] = self._storage_buffer\n        return result\n\n    def save(self, file_path: str):\n        save_file(self.state_dict(), file_path)\n\n    def _get_next_dataset_tokens(self) -&gt; torch.Tensor:\n        device = self.device\n        if not self.is_dataset_tokenized:\n            s = next(self.iterable_dataset)[self.tokens_column]\n            tokens = self.model.to_tokens(\n                s,\n                truncate=True,\n                move_to_device=True,\n                prepend_bos=self.prepend_bos,\n            ).squeeze(0)\n            assert (\n                len(tokens.shape) == 1\n            ), f\"tokens.shape should be 1D but was {tokens.shape}\"\n        else:\n            tokens = torch.tensor(\n                next(self.iterable_dataset)[self.tokens_column],\n                dtype=torch.long,\n                device=device,\n                requires_grad=False,\n            )\n            if (\n                not self.prepend_bos\n                and tokens[0] == self.model.tokenizer.bos_token_id  # type: ignore\n            ):\n                tokens = tokens[1:]\n        self.n_dataset_processed += 1\n        return tokens\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.activations_store.ActivationsStore.get_activations","title":"<code>get_activations(batch_tokens)</code>","text":"<p>Returns activations of shape (batches, context, num_layers, d_in)</p> <p>d_in may result from a concatenated head dimension.</p> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>def get_activations(self, batch_tokens: torch.Tensor):\n    \"\"\"\n    Returns activations of shape (batches, context, num_layers, d_in)\n\n    d_in may result from a concatenated head dimension.\n    \"\"\"\n    layers = self.hook_point_layers\n    act_names = [self.hook_point.format(layer=layer) for layer in layers]\n    hook_point_max_layer = max(layers)\n\n    layerwise_activations = self.model.run_with_cache(\n        batch_tokens,\n        names_filter=act_names,\n        stop_at_layer=hook_point_max_layer + 1,\n        prepend_bos=self.prepend_bos,\n        **self.model_kwargs,\n    )[1]\n\n    n_batches, n_context = batch_tokens.shape\n\n    stacked_activations = torch.zeros(\n        (n_batches, n_context, len(layers), self.d_in)\n    )\n\n    for i, act_name in enumerate(act_names):\n\n        if self.hook_point_head_index is not None:\n            stacked_activations[:, :, i] = layerwise_activations[act_name][\n                :, :, self.hook_point_head_index\n            ]\n        elif (\n            layerwise_activations[act_names[0]].ndim &gt; 3\n        ):  # if we have a head dimension\n            stacked_activations[:, :, i] = layerwise_activations[act_name].view(\n                n_batches, n_context, -1\n            )\n        else:\n            stacked_activations[:, :, i] = layerwise_activations[act_name]\n\n    return stacked_activations\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.activations_store.ActivationsStore.get_batch_tokens","title":"<code>get_batch_tokens(batch_size=None)</code>","text":"<p>Streams a batch of tokens from a dataset.</p> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>def get_batch_tokens(self, batch_size: int | None = None):\n    \"\"\"\n    Streams a batch of tokens from a dataset.\n    \"\"\"\n    if not batch_size:\n        batch_size = self.store_batch_size_prompts\n    context_size = self.context_size\n    device = self.device\n\n    batch_tokens = torch.zeros(\n        size=(0, context_size), device=device, dtype=torch.long, requires_grad=False\n    )\n\n    current_batch = []\n    current_length = 0\n\n    # pbar = tqdm(total=batch_size, desc=\"Filling batches\")\n    while batch_tokens.shape[0] &lt; batch_size:\n        tokens = self._get_next_dataset_tokens()\n        token_len = tokens.shape[0]\n\n        # TODO: Fix this so that we are limiting how many tokens we get from the same context.\n        assert self.model.tokenizer is not None  # keep pyright happy\n        while token_len &gt; 0 and batch_tokens.shape[0] &lt; batch_size:\n            # Space left in the current batch\n            space_left = context_size - current_length\n\n            # If the current tokens fit entirely into the remaining space\n            if token_len &lt;= space_left:\n                current_batch.append(tokens[:token_len])\n                current_length += token_len\n                break\n\n            else:\n                # Take as much as will fit\n                current_batch.append(tokens[:space_left])\n\n                # Remove used part, add BOS\n                tokens = tokens[space_left:]\n                token_len -= space_left\n\n                # only add BOS if it's not already the first token\n                if self.prepend_bos:\n                    bos_token_id_tensor = torch.tensor(\n                        [self.model.tokenizer.bos_token_id],\n                        device=tokens.device,\n                        dtype=torch.long,\n                    )\n                    if tokens[0] != bos_token_id_tensor:\n                        tokens = torch.cat(\n                            (\n                                bos_token_id_tensor,\n                                tokens,\n                            ),\n                            dim=0,\n                        )\n                        token_len += 1\n                current_length = context_size\n\n            # If a batch is full, concatenate and move to next batch\n            if current_length == context_size:\n                full_batch = torch.cat(current_batch, dim=0)\n                batch_tokens = torch.cat(\n                    (batch_tokens, full_batch.unsqueeze(0)), dim=0\n                )\n                current_batch = []\n                current_length = 0\n\n        # pbar.n = batch_tokens.shape[0]\n        # pbar.refresh()\n    return batch_tokens[:batch_size]\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.activations_store.ActivationsStore.get_data_loader","title":"<code>get_data_loader()</code>","text":"<p>Return a torch.utils.dataloader which you can get batches from.</p> <p>Should automatically refill the buffer when it gets to n % full. (better mixing if you refill and shuffle regularly).</p> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>def get_data_loader(\n    self,\n) -&gt; Iterator[Any]:\n    \"\"\"\n    Return a torch.utils.dataloader which you can get batches from.\n\n    Should automatically refill the buffer when it gets to n % full.\n    (better mixing if you refill and shuffle regularly).\n\n    \"\"\"\n\n    batch_size = self.train_batch_size_tokens\n\n    # 1. # create new buffer by mixing stored and new buffer\n    mixing_buffer = torch.cat(\n        [self.get_buffer(self.n_batches_in_buffer // 2), self.storage_buffer],\n        dim=0,\n    )\n\n    mixing_buffer = mixing_buffer[torch.randperm(mixing_buffer.shape[0])]\n\n    # 2.  put 50 % in storage\n    self._storage_buffer = mixing_buffer[: mixing_buffer.shape[0] // 2]\n\n    # 3. put other 50 % in a dataloader\n    dataloader = iter(\n        DataLoader(\n            # TODO: seems like a typing bug?\n            cast(Any, mixing_buffer[mixing_buffer.shape[0] // 2 :]),\n            batch_size=batch_size,\n            shuffle=True,\n        )\n    )\n\n    return dataloader\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.activations_store.ActivationsStore.next_batch","title":"<code>next_batch()</code>","text":"<p>Get the next batch from the current DataLoader. If the DataLoader is exhausted, refill the buffer and create a new DataLoader.</p> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>def next_batch(self):\n    \"\"\"\n    Get the next batch from the current DataLoader.\n    If the DataLoader is exhausted, refill the buffer and create a new DataLoader.\n    \"\"\"\n    try:\n        # Try to get the next batch\n        return next(self.dataloader)\n    except StopIteration:\n        # If the DataLoader is exhausted, create a new one\n        self._dataloader = self.get_data_loader()\n        return next(self.dataloader)\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.activations_store.ActivationsStore.save_buffer","title":"<code>save_buffer(buffer, path)</code>","text":"<p>Used by cached activations runner to save a buffer to disk. For reuse by later workflows.</p> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>def save_buffer(self, buffer: torch.Tensor, path: str):\n    \"\"\"\n    Used by cached activations runner to save a buffer to disk.\n    For reuse by later workflows.\n    \"\"\"\n    save_file({\"activations\": buffer}, path)\n</code></pre>"},{"location":"reference/misc/","title":"Misc","text":"<p>Took the LR scheduler from my previous work: https://github.com/jbloomAus/DecisionTransformerInterpretability/blob/ee55df35cdb92e81d689c72fb9dd5a7252893363/src/decision_transformer/utils.py#L425</p>"},{"location":"reference/misc/#sae_lens.training.config.CacheActivationsRunnerConfig","title":"<code>CacheActivationsRunnerConfig</code>  <code>dataclass</code>","text":"<p>Configuration for caching activations of an LLM.</p> Source code in <code>sae_lens/training/config.py</code> <pre><code>@dataclass\nclass CacheActivationsRunnerConfig:\n    \"\"\"\n    Configuration for caching activations of an LLM.\n    \"\"\"\n\n    # Data Generating Function (Model + Training Distibuion)\n    model_name: str = \"gelu-2l\"\n    model_class_name: str = \"HookedTransformer\"\n    hook_point: str = \"blocks.{layer}.hook_mlp_out\"\n    hook_point_layer: int | list[int] = 0\n    hook_point_head_index: Optional[int] = None\n    dataset_path: str = \"NeelNanda/c4-tokenized-2b\"\n    streaming: bool = True\n    is_dataset_tokenized: bool = True\n    context_size: int = 128\n    new_cached_activations_path: Optional[str] = (\n        None  # Defaults to \"activations/{dataset}/{model}/{full_hook_name}_{hook_point_head_index}\"\n    )\n    # dont' specify this since you don't want to load from disk with the cache runner.\n    cached_activations_path: Optional[str] = None\n    # SAE Parameters\n    d_in: int = 512\n\n    # Activation Store Parameters\n    n_batches_in_buffer: int = 20\n    training_tokens: int = 2_000_000\n    store_batch_size_prompts: int = 32\n    train_batch_size_tokens: int = 4096\n    normalize_activations: bool = False\n\n    # Misc\n    device: str | torch.device = \"cpu\"\n    seed: int = 42\n    dtype: str | torch.dtype = \"float32\"\n    prepend_bos: bool = True\n\n    # Activation caching stuff\n    shuffle_every_n_buffers: int = 10\n    n_shuffles_with_last_section: int = 10\n    n_shuffles_in_entire_dir: int = 10\n    n_shuffles_final: int = 100\n    model_kwargs: dict[str, Any] = field(default_factory=dict)\n    model_from_pretrained_kwargs: dict[str, Any] = field(default_factory=dict)\n\n    def __post_init__(self):\n        # Autofill cached_activations_path unless the user overrode it\n        if self.new_cached_activations_path is None:\n            self.new_cached_activations_path = _default_cached_activations_path(\n                self.dataset_path,\n                self.model_name,\n                self.hook_point,\n                self.hook_point_head_index,\n            )\n</code></pre>"},{"location":"reference/misc/#sae_lens.training.config.LanguageModelSAERunnerConfig","title":"<code>LanguageModelSAERunnerConfig</code>  <code>dataclass</code>","text":"<p>Configuration for training a sparse autoencoder on a language model.</p> Source code in <code>sae_lens/training/config.py</code> <pre><code>@dataclass\nclass LanguageModelSAERunnerConfig:\n    \"\"\"\n    Configuration for training a sparse autoencoder on a language model.\n    \"\"\"\n\n    # Data Generating Function (Model + Training Distibuion)\n    model_name: str = \"gelu-2l\"\n    model_class_name: str = \"HookedTransformer\"\n    hook_point: str = \"blocks.{layer}.hook_mlp_out\"\n    hook_point_eval: str = \"blocks.{layer}.attn.pattern\"\n    hook_point_layer: int | list[int] = 0\n    hook_point_head_index: Optional[int] = None\n    dataset_path: str = \"NeelNanda/c4-tokenized-2b\"\n    streaming: bool = True\n    is_dataset_tokenized: bool = True\n    context_size: int = 128\n    use_cached_activations: bool = False\n    cached_activations_path: Optional[str] = (\n        None  # Defaults to \"activations/{dataset}/{model}/{full_hook_name}_{hook_point_head_index}\"\n    )\n\n    # SAE Parameters\n    d_in: int = 512\n    d_sae: Optional[int] = None\n    b_dec_init_method: str = \"geometric_median\"\n    expansion_factor: int | list[int] = 4\n    activation_fn: str = \"relu\"  # relu, tanh-relu\n    normalize_sae_decoder: bool = True\n    noise_scale: float = 0.0\n    from_pretrained_path: Optional[str] = None\n    apply_b_dec_to_input: bool = True\n    decoder_orthogonal_init: bool = False\n    decoder_heuristic_init: bool = False\n    init_encoder_as_decoder_transpose: bool = False\n\n    # Activation Store Parameters\n    n_batches_in_buffer: int = 20\n    training_tokens: int = 2_000_000\n    finetuning_tokens: int = 0\n    store_batch_size_prompts: int = 32\n    train_batch_size_tokens: int = 4096\n    normalize_activations: bool = False\n\n    # Misc\n    device: str | torch.device = \"cpu\"\n    seed: int = 42\n    dtype: str | torch.dtype = \"float32\"  # type: ignore #\n    prepend_bos: bool = True\n\n    # Performance - see compilation section of lm_runner.py for info\n    autocast: bool = False  # autocast to autocast_dtype during training\n    compile_llm: bool = False  # use torch.compile on the LLM\n    llm_compilation_mode: str | None = None  # which torch.compile mode to use\n    compile_sae: bool = False  # use torch.compile on the SAE\n    sae_compilation_mode: str | None = None\n\n    # Training Parameters\n\n    ## Batch size\n    train_batch_size_tokens: int = 4096\n\n    ## Adam\n    adam_beta1: float | list[float] = 0\n    adam_beta2: float | list[float] = 0.999\n\n    ## Loss Function\n    mse_loss_normalization: Optional[str] = None\n    l1_coefficient: float | list[float] = 1e-3\n    lp_norm: float | list[float] = 1\n    scale_sparsity_penalty_by_decoder_norm: bool = False\n    l1_warm_up_steps: int | list[int] = 0\n\n    ## Learning Rate Schedule\n    lr: float | list[float] = 3e-4\n    lr_scheduler_name: str | list[str] = (\n        \"constant\"  # constant, cosineannealing, cosineannealingwarmrestarts\n    )\n    lr_warm_up_steps: int | list[int] = 500\n    lr_end: float | list[float] | None = (\n        None  # only used for cosine annealing, default is lr / 10\n    )\n    lr_decay_steps: int | list[int] = 0\n    n_restart_cycles: int | list[int] = 1  # used only for cosineannealingwarmrestarts\n\n    ## FineTuning\n    finetuning_method: Optional[str] = None  # scale, decoder or unrotated_decoder\n\n    # Resampling protocol args\n    use_ghost_grads: bool | list[bool] = (\n        False  # want to change this to true on some timeline.\n    )\n    feature_sampling_window: int = 2000\n    dead_feature_window: int = 1000  # unless this window is larger feature sampling,\n\n    dead_feature_threshold: float = 1e-8\n\n    # Evals\n    n_eval_batches: int = 10\n    eval_batch_size_prompts: int | None = None  # useful if evals cause OOM\n\n    # WANDB\n    log_to_wandb: bool = True\n    log_activations_store_to_wandb: bool = False\n    log_optimizer_state_to_wandb: bool = False\n    wandb_project: str = \"mats_sae_training_language_model\"\n    wandb_id: Optional[str] = None\n    run_name: Optional[str] = None\n    wandb_entity: Optional[str] = None\n    wandb_log_frequency: int = 10\n    eval_every_n_wandb_logs: int = 100  # logs every 1000 steps.\n\n    # Misc\n    resume: bool = False\n    n_checkpoints: int = 0\n    checkpoint_path: str = \"checkpoints\"\n    verbose: bool = True\n    model_kwargs: dict[str, Any] = field(default_factory=dict)\n    model_from_pretrained_kwargs: dict[str, Any] = field(default_factory=dict)\n    sae_lens_version: str = field(default_factory=lambda: __version__)\n    sae_lens_training_version: str = field(default_factory=lambda: __version__)\n\n    def __post_init__(self):\n        if self.use_cached_activations and self.cached_activations_path is None:\n            self.cached_activations_path = _default_cached_activations_path(\n                self.dataset_path,\n                self.model_name,\n                self.hook_point,\n                self.hook_point_head_index,\n            )\n\n        if not isinstance(self.expansion_factor, list):\n            self.d_sae = self.d_in * self.expansion_factor\n        self.tokens_per_buffer = (\n            self.train_batch_size_tokens * self.context_size * self.n_batches_in_buffer\n        )\n\n        if self.run_name is None:\n            self.run_name = f\"{self.d_sae}-L1-{self.l1_coefficient}-LR-{self.lr}-Tokens-{self.training_tokens:3.3e}\"\n\n        if self.b_dec_init_method not in [\"geometric_median\", \"mean\", \"zeros\"]:\n            raise ValueError(\n                f\"b_dec_init_method must be geometric_median, mean, or zeros. Got {self.b_dec_init_method}\"\n            )\n\n        if self.normalize_sae_decoder and self.decoder_heuristic_init:\n            raise ValueError(\n                \"You can't normalize the decoder and use heuristic initialization.\"\n            )\n\n        if self.normalize_sae_decoder and self.scale_sparsity_penalty_by_decoder_norm:\n            raise ValueError(\n                \"Weighting loss by decoder norm makes no sense if you are normalizing the decoder weight norms to 1\"\n            )\n\n        if isinstance(self.dtype, str) and self.dtype not in DTYPE_MAP:\n            raise ValueError(\n                f\"dtype must be one of {list(DTYPE_MAP.keys())}. Got {self.dtype}\"\n            )\n        elif isinstance(self.dtype, str):\n            self.dtype: torch.dtype = DTYPE_MAP[self.dtype]\n\n        # if we use decoder fine tuning, we can't be applying b_dec to the input\n        if (self.finetuning_method == \"decoder\") and (self.apply_b_dec_to_input):\n            raise ValueError(\n                \"If we are fine tuning the decoder, we can't be applying b_dec to the input.\\nSet apply_b_dec_to_input to False.\"\n            )\n\n        self.device: str | torch.device = torch.device(self.device)\n\n        if self.lr_end is None:\n            if isinstance(self.lr, list):\n                self.lr_end = [lr / 10 for lr in self.lr]\n            else:\n                self.lr_end = self.lr / 10\n        unique_id = self.wandb_id\n        if unique_id is None:\n            unique_id = cast(\n                Any, wandb\n            ).util.generate_id()  # not sure why this type is erroring\n        self.checkpoint_path = f\"{self.checkpoint_path}/{unique_id}\"\n\n        if self.verbose:\n            print(\n                f\"Run name: {self.d_sae}-L1-{self.l1_coefficient}-LR-{self.lr}-Tokens-{self.training_tokens:3.3e}\"\n            )\n            # Print out some useful info:\n            n_tokens_per_buffer = (\n                self.store_batch_size_prompts\n                * self.context_size\n                * self.n_batches_in_buffer\n            )\n            print(f\"n_tokens_per_buffer (millions): {n_tokens_per_buffer / 10 ** 6}\")\n            n_contexts_per_buffer = (\n                self.store_batch_size_prompts * self.n_batches_in_buffer\n            )\n            print(\n                f\"Lower bound: n_contexts_per_buffer (millions): {n_contexts_per_buffer / 10 ** 6}\"\n            )\n\n            total_training_steps = (\n                self.training_tokens + self.finetuning_tokens\n            ) // self.train_batch_size_tokens\n            print(f\"Total training steps: {total_training_steps}\")\n\n            total_wandb_updates = total_training_steps // self.wandb_log_frequency\n            print(f\"Total wandb updates: {total_wandb_updates}\")\n\n            # how many times will we sample dead neurons?\n            # assert self.dead_feature_window &lt;= self.feature_sampling_window, \"dead_feature_window must be smaller than feature_sampling_window\"\n            n_feature_window_samples = (\n                total_training_steps // self.feature_sampling_window\n            )\n            print(\n                f\"n_tokens_per_feature_sampling_window (millions): {(self.feature_sampling_window * self.context_size * self.train_batch_size_tokens) / 10 ** 6}\"\n            )\n            print(\n                f\"n_tokens_per_dead_feature_window (millions): {(self.dead_feature_window * self.context_size * self.train_batch_size_tokens) / 10 ** 6}\"\n            )\n            print(\n                f\"We will reset the sparsity calculation {n_feature_window_samples} times.\"\n            )\n            # print(\"Number tokens in dead feature calculation window: \", self.dead_feature_window * self.train_batch_size_tokens)\n            print(\n                f\"Number tokens in sparsity calculation window: {self.feature_sampling_window * self.train_batch_size_tokens:.2e}\"\n            )\n\n        if not isinstance(self.use_ghost_grads, list) and self.use_ghost_grads:\n            print(\"Using Ghost Grads.\")\n\n    def get_checkpoints_by_step(self) -&gt; tuple[dict[int, str], bool]:\n        \"\"\"\n        Returns (dict, is_done)\n        where dict is [steps] = path\n        for each checkpoint, and\n        is_done is True if there is a \"final_{steps}\" checkpoint\n        \"\"\"\n        is_done = False\n        checkpoints = [\n            f\n            for f in os.listdir(self.checkpoint_path)\n            if os.path.isdir(os.path.join(self.checkpoint_path, f))\n        ]\n        mapped_to_steps = {}\n        for c in checkpoints:\n            try:\n                steps = int(c)\n            except ValueError:\n                if c.startswith(\"final\"):\n                    steps = int(c.split(\"_\")[1])\n                    is_done = True\n                else:\n                    continue  # ignore this directory\n            full_path = os.path.join(self.checkpoint_path, c)\n            mapped_to_steps[steps] = full_path\n        return mapped_to_steps, is_done\n\n    def get_resume_checkpoint_path(self) -&gt; str:\n        \"\"\"\n        Gets the checkpoint path with the most steps\n        raises StopIteration if the model is done (there is a final_{steps} directoryh\n        raises FileNotFoundError if there are no checkpoints found\n        \"\"\"\n        mapped_to_steps, is_done = self.get_checkpoints_by_step()\n        if is_done:\n            raise StopIteration(\"Finished training model\")\n        if len(mapped_to_steps) == 0:\n            raise FileNotFoundError(\"no checkpoints available to resume from\")\n        else:\n            max_step = max(list(mapped_to_steps.keys()))\n            checkpoint_dir = mapped_to_steps[max_step]\n            print(f\"resuming from step {max_step} at path {checkpoint_dir}\")\n            return mapped_to_steps[max_step]\n</code></pre>"},{"location":"reference/misc/#sae_lens.training.config.LanguageModelSAERunnerConfig.get_checkpoints_by_step","title":"<code>get_checkpoints_by_step()</code>","text":"<p>Returns (dict, is_done) where dict is [steps] = path for each checkpoint, and is_done is True if there is a \"final_{steps}\" checkpoint</p> Source code in <code>sae_lens/training/config.py</code> <pre><code>def get_checkpoints_by_step(self) -&gt; tuple[dict[int, str], bool]:\n    \"\"\"\n    Returns (dict, is_done)\n    where dict is [steps] = path\n    for each checkpoint, and\n    is_done is True if there is a \"final_{steps}\" checkpoint\n    \"\"\"\n    is_done = False\n    checkpoints = [\n        f\n        for f in os.listdir(self.checkpoint_path)\n        if os.path.isdir(os.path.join(self.checkpoint_path, f))\n    ]\n    mapped_to_steps = {}\n    for c in checkpoints:\n        try:\n            steps = int(c)\n        except ValueError:\n            if c.startswith(\"final\"):\n                steps = int(c.split(\"_\")[1])\n                is_done = True\n            else:\n                continue  # ignore this directory\n        full_path = os.path.join(self.checkpoint_path, c)\n        mapped_to_steps[steps] = full_path\n    return mapped_to_steps, is_done\n</code></pre>"},{"location":"reference/misc/#sae_lens.training.config.LanguageModelSAERunnerConfig.get_resume_checkpoint_path","title":"<code>get_resume_checkpoint_path()</code>","text":"<p>Gets the checkpoint path with the most steps raises StopIteration if the model is done (there is a final_{steps} directoryh raises FileNotFoundError if there are no checkpoints found</p> Source code in <code>sae_lens/training/config.py</code> <pre><code>def get_resume_checkpoint_path(self) -&gt; str:\n    \"\"\"\n    Gets the checkpoint path with the most steps\n    raises StopIteration if the model is done (there is a final_{steps} directoryh\n    raises FileNotFoundError if there are no checkpoints found\n    \"\"\"\n    mapped_to_steps, is_done = self.get_checkpoints_by_step()\n    if is_done:\n        raise StopIteration(\"Finished training model\")\n    if len(mapped_to_steps) == 0:\n        raise FileNotFoundError(\"no checkpoints available to resume from\")\n    else:\n        max_step = max(list(mapped_to_steps.keys()))\n        checkpoint_dir = mapped_to_steps[max_step]\n        print(f\"resuming from step {max_step} at path {checkpoint_dir}\")\n        return mapped_to_steps[max_step]\n</code></pre>"},{"location":"reference/misc/#sae_lens.training.session_loader.LMSparseAutoencoderSessionloader","title":"<code>LMSparseAutoencoderSessionloader</code>","text":"<p>Responsible for loading all required artifacts and files for training a sparse autoencoder on a language model or analysing a pretraining autoencoder</p> Source code in <code>sae_lens/training/session_loader.py</code> <pre><code>class LMSparseAutoencoderSessionloader:\n    \"\"\"\n    Responsible for loading all required\n    artifacts and files for training\n    a sparse autoencoder on a language model\n    or analysing a pretraining autoencoder\n    \"\"\"\n\n    def __init__(self, cfg: LanguageModelSAERunnerConfig):\n        self.cfg = cfg\n\n    def load_sae_training_group_session(\n        self,\n    ) -&gt; Tuple[HookedRootModule, SparseAutoencoderDictionary, ActivationsStore]:\n        \"\"\"\n        Loads a session for training a sparse autoencoder on a language model.\n        \"\"\"\n\n        model = self.get_model(self.cfg.model_name)\n        model.to(self.cfg.device)\n        activations_loader = ActivationsStore.from_config(\n            model,\n            self.cfg,\n        )\n\n        sae_group = SparseAutoencoderDictionary(self.cfg)\n\n        return model, sae_group, activations_loader\n\n    @classmethod\n    def load_pretrained_sae(\n        cls, path: str, device: str = \"cpu\"\n    ) -&gt; Tuple[HookedRootModule, SparseAutoencoderDictionary, ActivationsStore]:\n        \"\"\"\n        Loads a session for analysing a pretrained sparse autoencoder.\n        \"\"\"\n\n        # load the SAE\n        sparse_autoencoders = SparseAutoencoderDictionary.load_from_pretrained(\n            path, device\n        )\n        first_sparse_autoencoder_cfg = next(iter(sparse_autoencoders))[1].cfg\n\n        # load the model, SAE and activations loader with it.\n        session_loader = cls(first_sparse_autoencoder_cfg)\n        model, _, activations_loader = session_loader.load_sae_training_group_session()\n\n        return model, sparse_autoencoders, activations_loader\n\n    def get_model(self, model_name: str) -&gt; HookedRootModule:\n        \"\"\"\n        Loads a model from transformer lens.\n\n        Abstracted to allow for easy modification.\n        \"\"\"\n\n        # Todo: add check that model_name is valid\n\n        model = load_model(\n            self.cfg.model_class_name,\n            model_name,\n            device=self.cfg.device,\n            model_from_pretrained_kwargs=self.cfg.model_from_pretrained_kwargs,\n        )\n        return model\n</code></pre>"},{"location":"reference/misc/#sae_lens.training.session_loader.LMSparseAutoencoderSessionloader.get_model","title":"<code>get_model(model_name)</code>","text":"<p>Loads a model from transformer lens.</p> <p>Abstracted to allow for easy modification.</p> Source code in <code>sae_lens/training/session_loader.py</code> <pre><code>def get_model(self, model_name: str) -&gt; HookedRootModule:\n    \"\"\"\n    Loads a model from transformer lens.\n\n    Abstracted to allow for easy modification.\n    \"\"\"\n\n    # Todo: add check that model_name is valid\n\n    model = load_model(\n        self.cfg.model_class_name,\n        model_name,\n        device=self.cfg.device,\n        model_from_pretrained_kwargs=self.cfg.model_from_pretrained_kwargs,\n    )\n    return model\n</code></pre>"},{"location":"reference/misc/#sae_lens.training.session_loader.LMSparseAutoencoderSessionloader.load_pretrained_sae","title":"<code>load_pretrained_sae(path, device='cpu')</code>  <code>classmethod</code>","text":"<p>Loads a session for analysing a pretrained sparse autoencoder.</p> Source code in <code>sae_lens/training/session_loader.py</code> <pre><code>@classmethod\ndef load_pretrained_sae(\n    cls, path: str, device: str = \"cpu\"\n) -&gt; Tuple[HookedRootModule, SparseAutoencoderDictionary, ActivationsStore]:\n    \"\"\"\n    Loads a session for analysing a pretrained sparse autoencoder.\n    \"\"\"\n\n    # load the SAE\n    sparse_autoencoders = SparseAutoencoderDictionary.load_from_pretrained(\n        path, device\n    )\n    first_sparse_autoencoder_cfg = next(iter(sparse_autoencoders))[1].cfg\n\n    # load the model, SAE and activations loader with it.\n    session_loader = cls(first_sparse_autoencoder_cfg)\n    model, _, activations_loader = session_loader.load_sae_training_group_session()\n\n    return model, sparse_autoencoders, activations_loader\n</code></pre>"},{"location":"reference/misc/#sae_lens.training.session_loader.LMSparseAutoencoderSessionloader.load_sae_training_group_session","title":"<code>load_sae_training_group_session()</code>","text":"<p>Loads a session for training a sparse autoencoder on a language model.</p> Source code in <code>sae_lens/training/session_loader.py</code> <pre><code>def load_sae_training_group_session(\n    self,\n) -&gt; Tuple[HookedRootModule, SparseAutoencoderDictionary, ActivationsStore]:\n    \"\"\"\n    Loads a session for training a sparse autoencoder on a language model.\n    \"\"\"\n\n    model = self.get_model(self.cfg.model_name)\n    model.to(self.cfg.device)\n    activations_loader = ActivationsStore.from_config(\n        model,\n        self.cfg,\n    )\n\n    sae_group = SparseAutoencoderDictionary(self.cfg)\n\n    return model, sae_group, activations_loader\n</code></pre>"},{"location":"reference/misc/#sae_lens.training.optim.L1Scheduler","title":"<code>L1Scheduler</code>","text":"Source code in <code>sae_lens/training/optim.py</code> <pre><code>class L1Scheduler:\n\n    def __init__(\n        self,\n        l1_warm_up_steps: float,\n        total_steps: int,\n        sparse_autoencoder: SparseAutoencoder,\n    ):\n\n        self.type = type\n        self.l1_warmup_steps = l1_warm_up_steps\n        self.total_steps = total_steps\n        self.sparse_autoencoder = sparse_autoencoder\n        self.final_l1_value = sparse_autoencoder.cfg.l1_coefficient\n        self.current_step = 0\n        assert isinstance(self.final_l1_value, float | int)\n\n        # assume using warm-up\n        if self.l1_warmup_steps != 0:\n            sparse_autoencoder.l1_coefficient = 0.0\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"L1Scheduler(final_l1_value={self.final_l1_value}, \"\n            f\"l1_warmup_steps={self.l1_warmup_steps}, \"\n            f\"total_steps={self.total_steps})\"\n        )\n\n    def step(self):\n        \"\"\"\n        Updates the l1 coefficient of the sparse autoencoder.\n        \"\"\"\n        step = self.current_step\n        if step &lt; self.l1_warmup_steps:\n            self.sparse_autoencoder.l1_coefficient = self.final_l1_value * (\n                (1 + step) / self.l1_warmup_steps\n            )  # type: ignore\n        else:\n            self.sparse_autoencoder.l1_coefficient = self.final_l1_value  # type: ignore\n\n        self.current_step += 1\n</code></pre>"},{"location":"reference/misc/#sae_lens.training.optim.L1Scheduler.step","title":"<code>step()</code>","text":"<p>Updates the l1 coefficient of the sparse autoencoder.</p> Source code in <code>sae_lens/training/optim.py</code> <pre><code>def step(self):\n    \"\"\"\n    Updates the l1 coefficient of the sparse autoencoder.\n    \"\"\"\n    step = self.current_step\n    if step &lt; self.l1_warmup_steps:\n        self.sparse_autoencoder.l1_coefficient = self.final_l1_value * (\n            (1 + step) / self.l1_warmup_steps\n        )  # type: ignore\n    else:\n        self.sparse_autoencoder.l1_coefficient = self.final_l1_value  # type: ignore\n\n    self.current_step += 1\n</code></pre>"},{"location":"reference/misc/#sae_lens.training.optim.get_lr_scheduler","title":"<code>get_lr_scheduler(scheduler_name, optimizer, training_steps, lr, warm_up_steps, decay_steps, lr_end, num_cycles)</code>","text":"<p>Loosely based on this, seemed simpler write this than import transformers: https://huggingface.co/docs/transformers/main_classes/optimizer_schedules</p> <p>Parameters:</p> Name Type Description Default <code>scheduler_name</code> <code>str</code> <p>Name of the scheduler to use, one of \"constant\", \"cosineannealing\", \"cosineannealingwarmrestarts\"</p> required <code>optimizer</code> <code>Optimizer</code> <p>Optimizer to use</p> required <code>training_steps</code> <code>int</code> <p>Total number of training steps</p> required <code>warm_up_steps</code> <code>int</code> <p>Number of linear warm up steps. Defaults to 0.</p> required <code>decay_steps</code> <code>int</code> <p>Number of linear decay steps to 0. Defaults to 0.</p> required <code>num_cycles</code> <code>int</code> <p>Number of cycles for cosine annealing with warm restarts. Defaults to 1.</p> required <code>lr_end</code> <code>float</code> <p>Final learning rate multiplier before decay. Defaults to 0.0.</p> required Source code in <code>sae_lens/training/optim.py</code> <pre><code>def get_lr_scheduler(\n    scheduler_name: str,\n    optimizer: optim.Optimizer,\n    training_steps: int,\n    lr: float,\n    warm_up_steps: int,\n    decay_steps: int,\n    lr_end: float,\n    num_cycles: int,\n) -&gt; lr_scheduler.LRScheduler:\n    \"\"\"\n    Loosely based on this, seemed simpler write this than import\n    transformers: https://huggingface.co/docs/transformers/main_classes/optimizer_schedules\n\n    Args:\n        scheduler_name (str): Name of the scheduler to use, one of \"constant\", \"cosineannealing\", \"cosineannealingwarmrestarts\"\n        optimizer (optim.Optimizer): Optimizer to use\n        training_steps (int): Total number of training steps\n        warm_up_steps (int, optional): Number of linear warm up steps. Defaults to 0.\n        decay_steps (int, optional): Number of linear decay steps to 0. Defaults to 0.\n        num_cycles (int, optional): Number of cycles for cosine annealing with warm restarts. Defaults to 1.\n        lr_end (float, optional): Final learning rate multiplier before decay. Defaults to 0.0.\n    \"\"\"\n    base_scheduler_steps = training_steps - warm_up_steps - decay_steps\n    norm_scheduler_name = scheduler_name.lower()\n    main_scheduler = _get_main_lr_scheduler(\n        norm_scheduler_name,\n        optimizer,\n        steps=base_scheduler_steps,\n        lr_end=lr_end,\n        num_cycles=num_cycles,\n    )\n    if norm_scheduler_name == \"constant\":\n        # constant scheduler ignores lr_end, so decay needs to start at lr\n        lr_end = lr\n    schedulers: list[lr_scheduler.LRScheduler] = []\n    milestones: list[int] = []\n    if warm_up_steps &gt; 0:\n        schedulers.append(\n            lr_scheduler.LinearLR(\n                optimizer,\n                start_factor=1 / warm_up_steps,\n                end_factor=1.0,\n                total_iters=warm_up_steps - 1,\n            ),\n        )\n        milestones.append(warm_up_steps)\n    schedulers.append(main_scheduler)\n    if decay_steps &gt; 0:\n        if lr_end == 0.0:\n            raise ValueError(\n                \"Cannot have decay_steps with lr_end=0.0, this would decay from 0 to 0 and be a waste.\"\n            )\n        schedulers.append(\n            lr_scheduler.LinearLR(\n                optimizer,\n                start_factor=lr_end / lr,\n                end_factor=0.0,\n                total_iters=decay_steps,\n            ),\n        )\n        milestones.append(training_steps - decay_steps)\n    return lr_scheduler.SequentialLR(\n        schedulers=schedulers,\n        optimizer=optimizer,\n        milestones=milestones,\n    )\n</code></pre>"},{"location":"reference/runners/","title":"Runners","text":""},{"location":"reference/runners/#sae_lens.training.lm_runner.language_model_sae_runner","title":"<code>language_model_sae_runner(cfg)</code>","text":"Source code in <code>sae_lens/training/lm_runner.py</code> <pre><code>def language_model_sae_runner(cfg: LanguageModelSAERunnerConfig):\n    \"\"\" \"\"\"\n    training_run_state = None\n    train_contexts = None\n\n    if cfg.resume:\n        try:\n            checkpoint_path = cfg.get_resume_checkpoint_path()\n            model = load_model(\n                model_class_name=cfg.model_class_name,\n                model_name=cfg.model_name,\n                device=cfg.device,\n            )\n            model.to(cfg.device)\n            (\n                training_run_state,\n                activations_loader,\n                sparse_autoencoder,\n                train_contexts,\n            ) = load_checkpoint(\n                checkpoint_path=checkpoint_path,\n                cfg=cfg,\n                model=model,\n                batch_size=cfg.train_batch_size_tokens,\n            )\n        # no checkpoints found, don't resume\n        except FileNotFoundError:\n            print(traceback.format_exc())\n            print(\"failed to find checkpoint to resume from, setting resume to False\")\n            cfg.resume = False\n\n    if not cfg.resume:\n        if cfg.from_pretrained_path is not None:\n            (\n                model,\n                sparse_autoencoder,\n                activations_loader,\n            ) = LMSparseAutoencoderSessionloader.load_pretrained_sae(\n                cfg.from_pretrained_path\n            )\n            cfg = sparse_autoencoder.cfg\n        else:\n            loader = LMSparseAutoencoderSessionloader(cfg)\n            model, sparse_autoencoder, activations_loader = (\n                loader.load_sae_training_group_session()\n            )\n\n    if cfg.log_to_wandb:\n        resume = None\n        if cfg.resume:\n            resume = \"allow\"\n        wandb.init(\n            project=cfg.wandb_project,\n            config=cast(Any, cfg),\n            name=cfg.run_name,\n            resume=resume,\n            id=cfg.wandb_id,\n        )\n\n    # Compile model and SAE\n    # torch.compile can provide significant speedups (10-20% in testing)\n    # using max-autotune gives the best speedups but:\n    # (a) increases VRAM usage,\n    # (b) can't be used on both SAE and LM (some issue with cudagraphs), and\n    # (c) takes some time to compile\n    # optimal settings seem to be:\n    # use max-autotune on SAE and max-autotune-no-cudagraphs on LM\n    # (also pylance seems to really hate this)\n    if cfg.compile_llm:\n        model = torch.compile(\n            model,  # pyright: ignore [reportPossiblyUnboundVariable]\n            mode=cfg.llm_compilation_mode,\n        )\n\n    if cfg.compile_sae:\n        for (\n            k\n        ) in (\n            sparse_autoencoder.autoencoders.keys()  # pyright: ignore [reportPossiblyUnboundVariable]\n        ):\n            sae = sparse_autoencoder.autoencoders[  # pyright: ignore [reportPossiblyUnboundVariable]\n                k\n            ]\n            sae = torch.compile(sae, mode=cfg.sae_compilation_mode)\n            sparse_autoencoder.autoencoders[k] = sae  # type: ignore # pyright: ignore [reportPossiblyUnboundVariable]\n\n    # train SAE\n    sparse_autoencoder = train_sae_group_on_language_model(\n        model=model,  # pyright: ignore [reportPossiblyUnboundVariable] # type: ignore\n        sae_group=sparse_autoencoder,  # pyright: ignore [reportPossiblyUnboundVariable]\n        activation_store=activations_loader,  # pyright: ignore [reportPossiblyUnboundVariable]\n        train_contexts=train_contexts,\n        training_run_state=training_run_state,\n        batch_size=cfg.train_batch_size_tokens,\n        n_checkpoints=cfg.n_checkpoints,\n        feature_sampling_window=cfg.feature_sampling_window,\n        use_wandb=cfg.log_to_wandb,\n        wandb_log_frequency=cfg.wandb_log_frequency,\n        eval_every_n_wandb_logs=cfg.eval_every_n_wandb_logs,\n        autocast=cfg.autocast,\n        n_eval_batches=cfg.n_eval_batches,\n        eval_batch_size_prompts=cfg.eval_batch_size_prompts,\n    ).sae_group\n\n    if cfg.log_to_wandb:\n        wandb.finish()\n\n    return sparse_autoencoder\n</code></pre>"},{"location":"reference/toy_models/","title":"Toy Models","text":"<p>https://www.lesswrong.com/posts/LnHowHgmrMbWtpkxx/intro-to-superposition-and-sparse-autoencoders-colab?fbclid=IwAR04OCGu_unvxezvDWkys9_6MJPEnXuu6GSqU6ScrMkAb1bvdSYFOeS35AY https://github.com/callummcdougall/sae-exercises-mats?fbclid=IwAR3qYAELbyD_x5IAYN4yCDFQzxXHeuH6CwMi_E7g4Qg6G1QXRNAYabQ4xGs</p>"},{"location":"reference/toy_models/#sae_lens.training.train_sae_on_toy_model.train_toy_sae","title":"<code>train_toy_sae(sparse_autoencoder, activation_store, batch_size=1024, feature_sampling_window=100, dead_feature_window=2000, dead_feature_threshold=1e-08, use_wandb=False, wandb_log_frequency=50)</code>","text":"<p>Takes an SAE and a bunch of activations and does a bunch of training steps</p> Source code in <code>sae_lens/training/train_sae_on_toy_model.py</code> <pre><code>def train_toy_sae(\n    sparse_autoencoder: SparseAutoencoder,\n    activation_store: torch.Tensor,  # TODO: this type seems strange / wrong\n    batch_size: int = 1024,\n    feature_sampling_window: int = 100,  # how many training steps between resampling the features / considiring neurons dead\n    dead_feature_window: int = 2000,  # how many training steps before a feature is considered dead\n    dead_feature_threshold: float = 1e-8,  # how infrequently a feature has to be active to be considered dead\n    use_wandb: bool = False,\n    wandb_log_frequency: int = 50,\n):\n    \"\"\"\n    Takes an SAE and a bunch of activations and does a bunch of training steps\n    \"\"\"\n\n    # TODO: this type seems strange\n    dataloader = iter(\n        DataLoader(cast(Any, activation_store), batch_size=batch_size, shuffle=True)\n    )\n    optimizer = torch.optim.Adam(sparse_autoencoder.parameters())\n    sparse_autoencoder.train()\n    frac_active_list = []  # track active features\n\n    n_training_steps = 0\n    n_training_tokens = 0\n\n    pbar = tqdm(dataloader, desc=\"Training SAE\")\n    for _, batch in enumerate(pbar):\n        batch = next(dataloader)\n        # Make sure the W_dec is still zero-norm\n        if sparse_autoencoder.normalize_sae_decoder:\n            sparse_autoencoder.set_decoder_norm_to_unit_norm()\n\n        # Forward and Backward Passes\n        optimizer.zero_grad()\n        sae_out, feature_acts, loss, mse_loss, l1_loss, _ = sparse_autoencoder(batch)\n        loss.backward()\n        if sparse_autoencoder.normalize_sae_decoder:\n            sparse_autoencoder.remove_gradient_parallel_to_decoder_directions()\n        optimizer.step()\n\n        n_training_tokens += batch_size\n\n        with torch.no_grad():\n            # Calculate the sparsities, and add it to a list\n            act_freq_scores = (feature_acts.abs() &gt; 0).float().sum(0)\n            frac_active_list.append(act_freq_scores)\n\n            if len(frac_active_list) &gt; feature_sampling_window:\n                frac_active_in_window = torch.stack(\n                    frac_active_list[-feature_sampling_window:], dim=0\n                )\n                feature_sparsity = frac_active_in_window.sum(0) / (\n                    feature_sampling_window * batch_size\n                )\n            else:\n                # use the whole list\n                frac_active_in_window = torch.stack(frac_active_list, dim=0)\n                feature_sparsity = frac_active_in_window.sum(0) / (\n                    len(frac_active_list) * batch_size\n                )\n\n            l0 = (feature_acts &gt; 0).float().sum(-1).mean()\n            l2_norm = torch.norm(feature_acts, dim=1).mean()\n\n            l2_norm_in = torch.norm(batch, dim=-1)\n            l2_norm_out = torch.norm(sae_out, dim=-1)\n            l2_norm_ratio = l2_norm_out / (1e-6 + l2_norm_in)\n\n            if use_wandb and ((n_training_steps + 1) % wandb_log_frequency == 0):\n                wandb.log(\n                    {\n                        \"details/n_training_tokens\": n_training_tokens,\n                        \"losses/mse_loss\": mse_loss.item(),\n                        \"losses/l1_loss\": l1_loss.item(),\n                        \"losses/overall_loss\": loss.item(),\n                        \"metrics/l0\": l0.item(),\n                        \"metrics/l2\": l2_norm.item(),\n                        \"metrics/l2_ratio\": l2_norm_ratio.mean().item(),\n                        \"sparsity/below_1e-5\": (feature_sparsity &lt; 1e-5)\n                        .float()\n                        .mean()\n                        .item(),\n                        \"sparsity/below_1e-6\": (feature_sparsity &lt; 1e-6)\n                        .float()\n                        .mean()\n                        .item(),\n                        \"sparsity/n_dead_features\": (\n                            feature_sparsity &lt; dead_feature_threshold\n                        )\n                        .float()\n                        .mean()\n                        .item(),\n                    },\n                    step=n_training_steps,\n                )\n\n                if (n_training_steps + 1) % (wandb_log_frequency * 100) == 0:\n                    log_feature_sparsity = torch.log10(feature_sparsity + 1e-8)\n                    wandb.log(\n                        {\n                            \"plots/feature_density_histogram\": wandb.Histogram(\n                                log_feature_sparsity.tolist()\n                            ),\n                        },\n                        step=n_training_steps,\n                    )\n\n            pbar.set_description(\n                f\"{n_training_steps}| MSE Loss {mse_loss.item():.3f} | L0 {l0.item():.3f}\"\n            )\n            pbar.update(batch_size)\n\n        # If we did checkpointing we'd do it here.\n\n        n_training_steps += 1\n\n    return sparse_autoencoder\n</code></pre>"},{"location":"reference/toy_models/#sae_lens.training.toy_model_runner.toy_model_sae_runner","title":"<code>toy_model_sae_runner(cfg)</code>","text":"<p>A runner for training an SAE on a toy model.</p> Source code in <code>sae_lens/training/toy_model_runner.py</code> <pre><code>def toy_model_sae_runner(cfg: ToyModelSAERunnerConfig):\n    \"\"\"\n    A runner for training an SAE on a toy model.\n    \"\"\"\n    # Toy Model Config\n    toy_model_cfg = ToyConfig(\n        n_features=cfg.n_features,\n        n_hidden=cfg.n_hidden,\n        n_correlated_pairs=cfg.n_correlated_pairs,\n        n_anticorrelated_pairs=cfg.n_anticorrelated_pairs,\n        feature_probability=cfg.feature_probability,\n    )\n\n    # Initialize Toy Model\n    model = ToyModel(\n        cfg=toy_model_cfg,\n        device=torch.device(cfg.device),\n    )\n\n    # Train the Toy Model\n    model.optimize(steps=cfg.model_training_steps)\n\n    # Generate Training Data\n    batch = model.generate_batch(cfg.total_training_tokens)\n    hidden = einops.einsum(\n        batch,\n        model.W,\n        \"batch_size features, hidden features -&gt; batch_size hidden\",\n    )\n\n    sparse_autoencoder = SparseAutoencoder(\n        cast(Any, cfg)  # TODO: the types are broken here\n    )  # config has the hyperparameters for the SAE\n\n    if cfg.log_to_wandb:\n        wandb.init(project=cfg.wandb_project, config=cast(Any, cfg))\n\n    sparse_autoencoder = train_toy_sae(\n        sparse_autoencoder,\n        activation_store=hidden.detach().squeeze(),\n        batch_size=cfg.train_batch_size,\n        feature_sampling_window=cfg.feature_sampling_window,\n        dead_feature_threshold=cfg.dead_feature_threshold,\n        use_wandb=cfg.log_to_wandb,\n        wandb_log_frequency=cfg.wandb_log_frequency,\n    )\n\n    if cfg.log_to_wandb:\n        wandb.finish()\n\n    return sparse_autoencoder\n</code></pre>"},{"location":"reference/toy_models/#sae_lens.training.toy_models.HookedToyModel","title":"<code>HookedToyModel</code>","text":"<p>             Bases: <code>HookedRootModule</code>, <code>ABC</code></p> Source code in <code>sae_lens/training/toy_models.py</code> <pre><code>class HookedToyModel(HookedRootModule, ABC):\n    def __init__(self, cfg: ToyConfig, device: torch.device = torch.device(\"cpu\")):\n        super().__init__()\n        self.cfg = cfg\n\n        feature_probability = cfg.feature_probability\n        if feature_probability is None:\n            feature_probability = t.ones(())\n        if isinstance(feature_probability, float):\n            feature_probability = t.tensor(feature_probability)\n        assert isinstance(\n            feature_probability, Tensor\n        )  # pyright can't seem to infer this\n        self.feature_probability = feature_probability.to(device).broadcast_to(\n            (cfg.n_features)\n        )\n\n        self.importance = _init_importance(cfg.importance, cfg.n_features, device)\n\n    @abstractmethod\n    def forward(self, features: Tensor, return_type: str) -&gt; Tensor:\n        \"\"\"Forward pass, to be implemented by subclasses\"\"\"\n\n    @abstractmethod\n    def calculate_loss(self, out: Tensor, batch: Tensor) -&gt; Tensor:\n        \"\"\"Loss calculation, to be implemented by subclasses\"\"\"\n\n    def generate_correlated_features(\n        self, batch_size: int, n_correlated_pairs: int\n    ) -&gt; Float[Tensor, \"batch_size features\"]:\n        \"\"\"\n        Generates a batch of correlated features.\n        Each output[i, j, 2k] and output[i, j, 2k + 1] are correlated, i.e. one is present iff the other is present.\n        \"\"\"\n        feat = t.rand(\n            (batch_size, 2 * n_correlated_pairs),\n            device=self.W.device,\n        )\n        feat_set_seeds = t.rand((batch_size, n_correlated_pairs), device=self.W.device)\n        feat_set_is_present = feat_set_seeds &lt;= self.feature_probability[[0]]\n        feat_is_present = einops.repeat(\n            feat_set_is_present,\n            \"batch features -&gt; batch (features pair)\",\n            pair=2,\n        )\n        return t.where(feat_is_present, feat, 0.0)\n\n    def generate_anticorrelated_features(\n        self, batch_size: int, n_anticorrelated_pairs: int\n    ) -&gt; Float[Tensor, \"batch_size features\"]:\n        \"\"\"\n        Generates a batch of anti-correlated features.\n        Each output[i, j, 2k] and output[i, j, 2k + 1] are anti-correlated, i.e. one is present iff the other is absent.\n        \"\"\"\n        feat = t.rand(\n            (batch_size, 2 * n_anticorrelated_pairs),\n            device=self.W.device,\n        )\n        feat_set_seeds = t.rand(\n            (batch_size, n_anticorrelated_pairs),\n            device=self.W.device,\n        )\n        first_feat_seeds = t.rand(\n            (batch_size, n_anticorrelated_pairs),\n            device=self.W.device,\n        )\n        feat_set_is_present = feat_set_seeds &lt;= 2 * self.feature_probability[[0]]\n        first_feat_is_present = first_feat_seeds &lt;= 0.5\n        first_feats = t.where(\n            feat_set_is_present &amp; first_feat_is_present,\n            feat[:, :n_anticorrelated_pairs],\n            0.0,\n        )\n        second_feats = t.where(\n            feat_set_is_present &amp; (~first_feat_is_present),\n            feat[:, n_anticorrelated_pairs:],\n            0.0,\n        )\n        return einops.rearrange(\n            t.concat([first_feats, second_feats], dim=-1),\n            \"batch (pair features) -&gt; batch (features pair)\",\n            pair=2,\n        )\n\n    def generate_uncorrelated_features(\n        self, batch_size: int, n_uncorrelated: int\n    ) -&gt; Float[Tensor, \"batch_size features\"]:\n        \"\"\"\n        Generates a batch of uncorrelated features.\n        \"\"\"\n        feat = t.rand((batch_size, n_uncorrelated), device=self.W.device)\n        feat_seeds = t.rand((batch_size, n_uncorrelated), device=self.W.device)\n        feat_is_present = feat_seeds &lt;= self.feature_probability[[0]]\n        return t.where(feat_is_present, feat, 0.0)\n\n    def generate_batch(self, batch_size: int) -&gt; Float[Tensor, \"batch_size features\"]:\n        \"\"\"\n        Generates a batch of data, with optional correlated &amp; anticorrelated features.\n        \"\"\"\n        n_uncorrelated = (\n            self.cfg.n_features\n            - 2 * self.cfg.n_correlated_pairs\n            - 2 * self.cfg.n_anticorrelated_pairs\n        )\n        data = []\n        if self.cfg.n_correlated_pairs &gt; 0:\n            data.append(\n                self.generate_correlated_features(\n                    batch_size, self.cfg.n_correlated_pairs\n                )\n            )\n        if self.cfg.n_anticorrelated_pairs &gt; 0:\n            data.append(\n                self.generate_anticorrelated_features(\n                    batch_size, self.cfg.n_anticorrelated_pairs\n                )\n            )\n        if n_uncorrelated &gt; 0:\n            data.append(self.generate_uncorrelated_features(batch_size, n_uncorrelated))\n        batch = t.cat(data, dim=-1)\n        return batch\n\n    def optimize(\n        self,\n        batch_size: int = 1024,\n        steps: int = 10_000,\n        log_freq: int = 100,\n        lr: float = 1e-3,\n        lr_scale: Callable[[int, int], float] = constant_lr,\n    ):\n        \"\"\"\n        Optimizes the model using the given hyperparameters.\n        \"\"\"\n        optimizer = t.optim.Adam(list(self.parameters()), lr=lr)\n\n        progress_bar = tqdm(range(steps), desc=\"Training Toy Model\")\n\n        for step in progress_bar:\n            # Update learning rate\n            step_lr = lr * lr_scale(step, steps)\n            for group in optimizer.param_groups:\n                group[\"lr\"] = step_lr\n\n            # Optimize\n            optimizer.zero_grad()\n            batch = self.generate_batch(batch_size)\n            out = self(batch)\n            loss = self.calculate_loss(out, batch)\n            loss.backward()\n            optimizer.step()\n\n            # Display progress bar\n            if step % log_freq == 0 or (step + 1 == steps):\n                progress_bar.set_postfix(loss=loss.item(), lr=step_lr)\n</code></pre>"},{"location":"reference/toy_models/#sae_lens.training.toy_models.HookedToyModel.calculate_loss","title":"<code>calculate_loss(out, batch)</code>  <code>abstractmethod</code>","text":"<p>Loss calculation, to be implemented by subclasses</p> Source code in <code>sae_lens/training/toy_models.py</code> <pre><code>@abstractmethod\ndef calculate_loss(self, out: Tensor, batch: Tensor) -&gt; Tensor:\n    \"\"\"Loss calculation, to be implemented by subclasses\"\"\"\n</code></pre>"},{"location":"reference/toy_models/#sae_lens.training.toy_models.HookedToyModel.forward","title":"<code>forward(features, return_type)</code>  <code>abstractmethod</code>","text":"<p>Forward pass, to be implemented by subclasses</p> Source code in <code>sae_lens/training/toy_models.py</code> <pre><code>@abstractmethod\ndef forward(self, features: Tensor, return_type: str) -&gt; Tensor:\n    \"\"\"Forward pass, to be implemented by subclasses\"\"\"\n</code></pre>"},{"location":"reference/toy_models/#sae_lens.training.toy_models.HookedToyModel.generate_anticorrelated_features","title":"<code>generate_anticorrelated_features(batch_size, n_anticorrelated_pairs)</code>","text":"<p>Generates a batch of anti-correlated features. Each output[i, j, 2k] and output[i, j, 2k + 1] are anti-correlated, i.e. one is present iff the other is absent.</p> Source code in <code>sae_lens/training/toy_models.py</code> <pre><code>def generate_anticorrelated_features(\n    self, batch_size: int, n_anticorrelated_pairs: int\n) -&gt; Float[Tensor, \"batch_size features\"]:\n    \"\"\"\n    Generates a batch of anti-correlated features.\n    Each output[i, j, 2k] and output[i, j, 2k + 1] are anti-correlated, i.e. one is present iff the other is absent.\n    \"\"\"\n    feat = t.rand(\n        (batch_size, 2 * n_anticorrelated_pairs),\n        device=self.W.device,\n    )\n    feat_set_seeds = t.rand(\n        (batch_size, n_anticorrelated_pairs),\n        device=self.W.device,\n    )\n    first_feat_seeds = t.rand(\n        (batch_size, n_anticorrelated_pairs),\n        device=self.W.device,\n    )\n    feat_set_is_present = feat_set_seeds &lt;= 2 * self.feature_probability[[0]]\n    first_feat_is_present = first_feat_seeds &lt;= 0.5\n    first_feats = t.where(\n        feat_set_is_present &amp; first_feat_is_present,\n        feat[:, :n_anticorrelated_pairs],\n        0.0,\n    )\n    second_feats = t.where(\n        feat_set_is_present &amp; (~first_feat_is_present),\n        feat[:, n_anticorrelated_pairs:],\n        0.0,\n    )\n    return einops.rearrange(\n        t.concat([first_feats, second_feats], dim=-1),\n        \"batch (pair features) -&gt; batch (features pair)\",\n        pair=2,\n    )\n</code></pre>"},{"location":"reference/toy_models/#sae_lens.training.toy_models.HookedToyModel.generate_batch","title":"<code>generate_batch(batch_size)</code>","text":"<p>Generates a batch of data, with optional correlated &amp; anticorrelated features.</p> Source code in <code>sae_lens/training/toy_models.py</code> <pre><code>def generate_batch(self, batch_size: int) -&gt; Float[Tensor, \"batch_size features\"]:\n    \"\"\"\n    Generates a batch of data, with optional correlated &amp; anticorrelated features.\n    \"\"\"\n    n_uncorrelated = (\n        self.cfg.n_features\n        - 2 * self.cfg.n_correlated_pairs\n        - 2 * self.cfg.n_anticorrelated_pairs\n    )\n    data = []\n    if self.cfg.n_correlated_pairs &gt; 0:\n        data.append(\n            self.generate_correlated_features(\n                batch_size, self.cfg.n_correlated_pairs\n            )\n        )\n    if self.cfg.n_anticorrelated_pairs &gt; 0:\n        data.append(\n            self.generate_anticorrelated_features(\n                batch_size, self.cfg.n_anticorrelated_pairs\n            )\n        )\n    if n_uncorrelated &gt; 0:\n        data.append(self.generate_uncorrelated_features(batch_size, n_uncorrelated))\n    batch = t.cat(data, dim=-1)\n    return batch\n</code></pre>"},{"location":"reference/toy_models/#sae_lens.training.toy_models.HookedToyModel.generate_correlated_features","title":"<code>generate_correlated_features(batch_size, n_correlated_pairs)</code>","text":"<p>Generates a batch of correlated features. Each output[i, j, 2k] and output[i, j, 2k + 1] are correlated, i.e. one is present iff the other is present.</p> Source code in <code>sae_lens/training/toy_models.py</code> <pre><code>def generate_correlated_features(\n    self, batch_size: int, n_correlated_pairs: int\n) -&gt; Float[Tensor, \"batch_size features\"]:\n    \"\"\"\n    Generates a batch of correlated features.\n    Each output[i, j, 2k] and output[i, j, 2k + 1] are correlated, i.e. one is present iff the other is present.\n    \"\"\"\n    feat = t.rand(\n        (batch_size, 2 * n_correlated_pairs),\n        device=self.W.device,\n    )\n    feat_set_seeds = t.rand((batch_size, n_correlated_pairs), device=self.W.device)\n    feat_set_is_present = feat_set_seeds &lt;= self.feature_probability[[0]]\n    feat_is_present = einops.repeat(\n        feat_set_is_present,\n        \"batch features -&gt; batch (features pair)\",\n        pair=2,\n    )\n    return t.where(feat_is_present, feat, 0.0)\n</code></pre>"},{"location":"reference/toy_models/#sae_lens.training.toy_models.HookedToyModel.generate_uncorrelated_features","title":"<code>generate_uncorrelated_features(batch_size, n_uncorrelated)</code>","text":"<p>Generates a batch of uncorrelated features.</p> Source code in <code>sae_lens/training/toy_models.py</code> <pre><code>def generate_uncorrelated_features(\n    self, batch_size: int, n_uncorrelated: int\n) -&gt; Float[Tensor, \"batch_size features\"]:\n    \"\"\"\n    Generates a batch of uncorrelated features.\n    \"\"\"\n    feat = t.rand((batch_size, n_uncorrelated), device=self.W.device)\n    feat_seeds = t.rand((batch_size, n_uncorrelated), device=self.W.device)\n    feat_is_present = feat_seeds &lt;= self.feature_probability[[0]]\n    return t.where(feat_is_present, feat, 0.0)\n</code></pre>"},{"location":"reference/toy_models/#sae_lens.training.toy_models.HookedToyModel.optimize","title":"<code>optimize(batch_size=1024, steps=10000, log_freq=100, lr=0.001, lr_scale=constant_lr)</code>","text":"<p>Optimizes the model using the given hyperparameters.</p> Source code in <code>sae_lens/training/toy_models.py</code> <pre><code>def optimize(\n    self,\n    batch_size: int = 1024,\n    steps: int = 10_000,\n    log_freq: int = 100,\n    lr: float = 1e-3,\n    lr_scale: Callable[[int, int], float] = constant_lr,\n):\n    \"\"\"\n    Optimizes the model using the given hyperparameters.\n    \"\"\"\n    optimizer = t.optim.Adam(list(self.parameters()), lr=lr)\n\n    progress_bar = tqdm(range(steps), desc=\"Training Toy Model\")\n\n    for step in progress_bar:\n        # Update learning rate\n        step_lr = lr * lr_scale(step, steps)\n        for group in optimizer.param_groups:\n            group[\"lr\"] = step_lr\n\n        # Optimize\n        optimizer.zero_grad()\n        batch = self.generate_batch(batch_size)\n        out = self(batch)\n        loss = self.calculate_loss(out, batch)\n        loss.backward()\n        optimizer.step()\n\n        # Display progress bar\n        if step % log_freq == 0 or (step + 1 == steps):\n            progress_bar.set_postfix(loss=loss.item(), lr=step_lr)\n</code></pre>"},{"location":"reference/toy_models/#sae_lens.training.toy_models.ReluOutputModel","title":"<code>ReluOutputModel</code>","text":"<p>             Bases: <code>HookedToyModel</code></p> <p>Anthropic's ReLU Output Model as described in the Toy Models paper:         https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-setup-model</p> Source code in <code>sae_lens/training/toy_models.py</code> <pre><code>class ReluOutputModel(HookedToyModel):\n    \"\"\"\n    Anthropic's ReLU Output Model as described in the Toy Models paper:\n            https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-setup-model\n    \"\"\"\n\n    W: Float[Tensor, \"n_hidden n_features\"]\n    b_final: Tensor\n    # Our linear map is x -&gt; ReLU(W.T @ W @ x + b_final)\n\n    def __init__(self, cfg: ToyConfig, device: torch.device = torch.device(\"cpu\")):\n        super().__init__(cfg)\n\n        self.W = nn.Parameter(\n            nn.init.xavier_normal_(t.empty((cfg.n_hidden, cfg.n_features)))\n        )\n        self.b_final = nn.Parameter(t.zeros((cfg.n_features)))\n        self.to(device)\n\n        # Add and setup hookpoints.\n        self.hook_hidden = HookPoint()\n        self.hook_out_prebias = HookPoint()\n        self.setup()\n\n    def forward(\n        self,\n        features: Float[Tensor, \"... features\"],\n        return_type: str = \"reconstruction\",\n    ) -&gt; Float[Tensor, \"... features\"]:\n        hidden = self.hook_hidden(\n            einops.einsum(\n                features,\n                self.W,\n                \"... features, hidden features -&gt; ... hidden\",\n            )\n        )\n        out = self.hook_out_prebias(\n            einops.einsum(\n                hidden,\n                self.W,\n                \"... hidden, hidden features -&gt; ... features\",\n            )\n        )\n        reconstructed = F.relu(out + self.b_final)\n\n        if return_type == \"loss\":\n            return self.calculate_loss(reconstructed, features)\n        elif return_type == \"reconstruction\":\n            return reconstructed\n        else:\n            raise ValueError(f\"Unknown return type: {return_type}\")\n\n    def calculate_loss(\n        self,\n        out: Float[Tensor, \"batch features\"],\n        batch: Float[Tensor, \"batch features\"],\n    ) -&gt; Float[Tensor, \"\"]:\n        \"\"\"\n        Calculates the loss for a given batch, using this loss described in the Toy Models paper:\n\n            https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-setup-loss\n\n        Note, `model.importance` is guaranteed to broadcast with the shape of `out` and `batch`.\n        \"\"\"\n        error = self.importance * ((batch - out) ** 2)\n        loss = einops.reduce(error, \"batch features -&gt; ()\", \"mean\").sum()\n        return loss\n</code></pre>"},{"location":"reference/toy_models/#sae_lens.training.toy_models.ReluOutputModel.calculate_loss","title":"<code>calculate_loss(out, batch)</code>","text":"<p>Calculates the loss for a given batch, using this loss described in the Toy Models paper:</p> <pre><code>https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-setup-loss\n</code></pre> <p>Note, <code>model.importance</code> is guaranteed to broadcast with the shape of <code>out</code> and <code>batch</code>.</p> Source code in <code>sae_lens/training/toy_models.py</code> <pre><code>def calculate_loss(\n    self,\n    out: Float[Tensor, \"batch features\"],\n    batch: Float[Tensor, \"batch features\"],\n) -&gt; Float[Tensor, \"\"]:\n    \"\"\"\n    Calculates the loss for a given batch, using this loss described in the Toy Models paper:\n\n        https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-setup-loss\n\n    Note, `model.importance` is guaranteed to broadcast with the shape of `out` and `batch`.\n    \"\"\"\n    error = self.importance * ((batch - out) ** 2)\n    loss = einops.reduce(error, \"batch features -&gt; ()\", \"mean\").sum()\n    return loss\n</code></pre>"},{"location":"reference/toy_models/#sae_lens.training.toy_models.ReluOutputModelCE","title":"<code>ReluOutputModelCE</code>","text":"<p>             Bases: <code>ReluOutputModel</code></p> <p>A variant of Anthropic's ReLU Output Model. This model is trained with a Cross Entropy loss instead of MSE loss. The model task is to identify which feature has the largest magnitude activation in the input. The model has an extra feature dimension which is set to a constant nonzero value, which allows for proper classification when all features are zero.</p> Source code in <code>sae_lens/training/toy_models.py</code> <pre><code>class ReluOutputModelCE(ReluOutputModel):\n    \"\"\"\n    A variant of Anthropic's ReLU Output Model.\n    This model is trained with a Cross Entropy loss instead of MSE loss.\n    The model task is to identify which feature has the largest magnitude activation in the input.\n    The model has an extra feature dimension which is set to a constant nonzero value,\n    which allows for proper classification when all features are zero.\n    \"\"\"\n\n    W: Float[Tensor, \"n_hidden n_features\"]\n    b_final: Tensor\n    # Our linear map is x -&gt; ReLU(W.T @ W @ x + b_final)\n\n    def __init__(\n        self,\n        cfg: ToyConfig,\n        device: torch.device = torch.device(\"cpu\"),\n        extra_feature_value: float = 1e-6,\n    ):\n        super().__init__(cfg)\n        self.extra_feature_value = extra_feature_value\n\n        self.W = nn.Parameter(\n            nn.init.xavier_normal_(t.empty((cfg.n_hidden, cfg.n_features + 1)))\n        )\n        self.b_final = nn.Parameter(t.zeros((cfg.n_features + 1)))\n        self.importance = _init_importance(cfg.importance, cfg.n_features + 1, device)\n        self.to(device)\n\n    def generate_batch(self, batch_size: int) -&gt; Float[Tensor, \"batch_size features\"]:\n        \"\"\"Adds an extra feature to the batch, which is set to a constant nonzero value.\"\"\"\n        batch = super().generate_batch(batch_size)\n        extra_feature = self.extra_feature_value * t.ones((batch_size, 1)).to(\n            batch.device\n        )\n        return t.cat((batch, extra_feature), dim=-1)\n\n    def calculate_loss(\n        self,\n        out: Float[Tensor, \"batch features\"],\n        batch: Float[Tensor, \"batch features\"],\n    ) -&gt; Float[Tensor, \"\"]:\n        \"\"\"\n        Calculates the loss for a given batch.\n        Loss is calculated using Cross Entropy loss, where the true probability distribution\n        is a one-hot encoding of the feature with the largest magnitude activation in the input.\n        Model outputs (raw logits) are weighted by importance before being passed through CE loss.\n\n        Note, `model.importance` is guaranteed to broadcast with the shape of `out` and `batch`.\n        \"\"\"\n        max_feat_indices = t.argmax(batch, dim=-1)\n        loss = F.cross_entropy(\n            (self.importance * out).squeeze(), max_feat_indices.squeeze()\n        )\n        return loss\n</code></pre>"},{"location":"reference/toy_models/#sae_lens.training.toy_models.ReluOutputModelCE.calculate_loss","title":"<code>calculate_loss(out, batch)</code>","text":"<p>Calculates the loss for a given batch. Loss is calculated using Cross Entropy loss, where the true probability distribution is a one-hot encoding of the feature with the largest magnitude activation in the input. Model outputs (raw logits) are weighted by importance before being passed through CE loss.</p> <p>Note, <code>model.importance</code> is guaranteed to broadcast with the shape of <code>out</code> and <code>batch</code>.</p> Source code in <code>sae_lens/training/toy_models.py</code> <pre><code>def calculate_loss(\n    self,\n    out: Float[Tensor, \"batch features\"],\n    batch: Float[Tensor, \"batch features\"],\n) -&gt; Float[Tensor, \"\"]:\n    \"\"\"\n    Calculates the loss for a given batch.\n    Loss is calculated using Cross Entropy loss, where the true probability distribution\n    is a one-hot encoding of the feature with the largest magnitude activation in the input.\n    Model outputs (raw logits) are weighted by importance before being passed through CE loss.\n\n    Note, `model.importance` is guaranteed to broadcast with the shape of `out` and `batch`.\n    \"\"\"\n    max_feat_indices = t.argmax(batch, dim=-1)\n    loss = F.cross_entropy(\n        (self.importance * out).squeeze(), max_feat_indices.squeeze()\n    )\n    return loss\n</code></pre>"},{"location":"reference/toy_models/#sae_lens.training.toy_models.ReluOutputModelCE.generate_batch","title":"<code>generate_batch(batch_size)</code>","text":"<p>Adds an extra feature to the batch, which is set to a constant nonzero value.</p> Source code in <code>sae_lens/training/toy_models.py</code> <pre><code>def generate_batch(self, batch_size: int) -&gt; Float[Tensor, \"batch_size features\"]:\n    \"\"\"Adds an extra feature to the batch, which is set to a constant nonzero value.\"\"\"\n    batch = super().generate_batch(batch_size)\n    extra_feature = self.extra_feature_value * t.ones((batch_size, 1)).to(\n        batch.device\n    )\n    return t.cat((batch, extra_feature), dim=-1)\n</code></pre>"},{"location":"reference/toy_models/#sae_lens.training.toy_models.parse_colors_for_superposition_plot","title":"<code>parse_colors_for_superposition_plot(colors, n_feats)</code>","text":"<p>There are lots of different ways colors can be represented in the superposition plot.</p> <p>This function unifies them all by turning colors into a list of lists of strings, i.e. one color for each instance &amp; feature.</p> Source code in <code>sae_lens/training/toy_models.py</code> <pre><code>def parse_colors_for_superposition_plot(\n    colors: Optional[Union[Tuple[int, int], List[str], Tensor]],\n    n_feats: int,\n) -&gt; List[str]:\n    \"\"\"\n    There are lots of different ways colors can be represented in the superposition plot.\n\n    This function unifies them all by turning colors into a list of lists of strings, i.e. one color for each instance &amp; feature.\n    \"\"\"\n    # If colors is a tensor, we assume it's the importances tensor, and we color according to a viridis color scheme\n    # if isinstance(colors, Tensor):\n    #     colors = t.broadcast_to(colors, (n_feats))\n    #     colors = [\n    #         [helper_get_viridis(v.item()) for v in colors_for_this_instance]\n    #         for colors_for_this_instance in colors\n    #     ]\n\n    # If colors is a tuple of ints, it's interpreted as number of correlated / anticorrelated pairs\n    if isinstance(colors, tuple):\n        n_corr, n_anti = colors\n        n_indep = n_feats - 2 * (n_corr - n_anti)\n        return (\n            [\"blue\", \"blue\", \"limegreen\", \"limegreen\", \"purple\", \"purple\"][: n_corr * 2]\n            + [\"red\", \"red\", \"orange\", \"orange\", \"brown\", \"brown\"][: n_anti * 2]\n            + [\"black\"] * n_indep\n        )\n\n    # If colors is a string, make all datapoints that color\n    elif isinstance(colors, str):\n        return [colors] * n_feats\n\n    # Lastly, if colors is None, make all datapoints black\n    elif colors is None:\n        return [\"black\"] * n_feats\n\n    assert isinstance(colors, list)\n    return colors\n</code></pre>"},{"location":"reference/toy_models/#sae_lens.training.toy_models.plot_features_in_2d","title":"<code>plot_features_in_2d(values, colors=None, title=None, subplot_titles=None, save=None, colab=False)</code>","text":"<p>Visualises superposition in 2D.</p> <p>If values is 3D, the first dimension is assumed to be timesteps, and an animation is created.</p> Source code in <code>sae_lens/training/toy_models.py</code> <pre><code>def plot_features_in_2d(\n    values: Float[Tensor, \"timesteps d_hidden feats\"],\n    colors: Optional[list[Any]] = None,  # shape [timesteps feats]\n    title: Optional[str | list[str]] = None,\n    subplot_titles: Optional[list[str] | list[list[str]]] = None,\n    save: Optional[str] = None,\n    colab: bool = False,\n):\n    \"\"\"\n    Visualises superposition in 2D.\n\n    If values is 3D, the first dimension is assumed to be timesteps, and an animation is created.\n    \"\"\"\n    # Convert values to 3D for consistency\n    if values.ndim == 2:\n        values = values.unsqueeze(0)\n    values = values.transpose(-1, -2)\n\n    # Get dimensions\n    n_timesteps, n_features, _ = values.shape\n\n    # If we have a large number of features per plot (i.e. we're plotting projections of data) then use smaller lines\n    linewidth, markersize = (1, 4) if (n_features &gt;= 25) else (2, 10)\n\n    # Convert colors to something which has 4D, if it is 3D (i.e. same colors for all timesteps)\n    if any(\n        [\n            colors is None,\n            isinstance(colors, list) and isinstance(colors[0], str),\n            (isinstance(colors, Tensor) or isinstance(colors, Arr))\n            and colors.ndim == 2,\n        ]\n    ):\n        colors = [colors for _ in range(values.shape[0])]\n    # Now that colors has length `timesteps` in some sense, we can convert it to lists of strings\n    assert colors is not None  # keep pyright happy\n    colors = [parse_colors_for_superposition_plot(c, n_features) for c in colors]\n\n    # Same for subplot titles &amp; titles\n    if subplot_titles is not None:\n        if isinstance(subplot_titles, list) and isinstance(subplot_titles[0], str):\n            subplot_titles = [\n                cast(list[str], subplot_titles) for _ in range(values.shape[0])\n            ]\n    if title is not None:\n        if isinstance(title, str):\n            title = [title for _ in range(values.shape[0])]\n\n    # Create a figure and axes\n    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n\n    # If there are titles, add more spacing for them\n    fig.subplots_adjust(bottom=0.2, top=0.9, left=0.1, right=0.9)\n    if title:\n        fig.subplots_adjust(top=0.8)\n\n    # Initialize lines and markers\n    ax.set_xlim(-1.5, 1.5)\n    ax.set_ylim(-1.5, 1.5)\n    ax.set_aspect(\"equal\", adjustable=\"box\")\n    lines = []\n    markers = []\n    for feature_idx in range(n_features):\n        (line,) = ax.plot([], [], color=colors[0][feature_idx], lw=linewidth)\n        (marker,) = ax.plot(\n            [],\n            [],\n            color=colors[0][feature_idx],\n            marker=\"o\",\n            markersize=markersize,\n        )\n        lines.append(line)\n        markers.append(marker)\n\n    def update(val: float):\n        # I think this doesn't work unless I at least reference the nonlocal slider object\n        # It works if I use t = int(val), so long as I put something like X = slider.val first. Idk why!\n        if n_timesteps &gt; 1:\n            _ = slider.val\n        t = int(val)\n        for feature_idx in range(n_features):\n            x, y = values[t, feature_idx].tolist()\n            lines[feature_idx].set_data([0, x], [0, y])\n            markers[feature_idx].set_data(x, y)\n            lines[feature_idx].set_color(colors[t][feature_idx])\n            markers[feature_idx].set_color(colors[t][feature_idx])\n        if title:\n            fig.suptitle(title[t], fontsize=15)\n        if subplot_titles:\n            ax.set_title(subplot_titles[t], fontsize=12)\n        fig.canvas.draw_idle()\n\n    def play(event: Any):\n        _ = slider.val\n        for i in range(n_timesteps):\n            update(i)\n            slider.set_val(i)\n            plt.pause(0.05)\n        fig.canvas.draw_idle()\n\n    if n_timesteps &gt; 1:\n        # Create the slider\n        ax_slider = plt.axes((0.15, 0.05, 0.7, 0.05), facecolor=\"lightgray\")\n        slider = Slider(\n            ax_slider, \"Time\", 0, n_timesteps - 1, valinit=0, valfmt=\"%1.0f\"\n        )\n\n        # Create the play button\n        # ax_button = plt.axes([0.8, 0.05, 0.08, 0.05], facecolor='lightgray')\n        # button = Button(ax_button, 'Play')\n\n        # Call the update function when the slider value is changed / button is clicked\n        slider.on_changed(update)\n        # button.on_clicked(play)\n\n        # Initialize the plot\n        play(0)\n    else:\n        update(0)\n\n    # Save\n    if isinstance(save, str):\n        ani = FuncAnimation(\n            fig, cast(Any, update), frames=n_timesteps, interval=0.04, repeat=False\n        )\n        ani.save(save, writer=\"pillow\", fps=25)\n    elif colab:\n        ani = FuncAnimation(\n            fig, cast(Any, update), frames=n_timesteps, interval=0.04, repeat=False\n        )\n        clear_output()\n        return ani\n</code></pre>"}]}